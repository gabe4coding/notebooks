{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Zero to RAG: An Incremental, Hands-On Notebook\n",
    "\n",
    "Welcome to this comprehensive tutorial on building a Retrieval-Augmented Generation (RAG) pipeline from scratch! In this notebook, you'll learn how to create a complete RAG system incrementally, starting with basic keyword search and progressing to sophisticated semantic retrieval with re-ranking.\n",
    "\n",
    "## What You'll Learn\n",
    "- Build retrieval systems using TF-IDF, BM25, and semantic embeddings\n",
    "- Implement hybrid retrieval combining lexical and semantic approaches\n",
    "- Apply rank fusion techniques (Reciprocal Rank Fusion)\n",
    "- Use cross-encoder re-ranking for improved precision\n",
    "- Create an end-to-end RAG pipeline with generation\n",
    "\n",
    "## What Gets Built\n",
    "By the end, you'll have a working RAG system that can answer questions about a synthetic knowledge base, complete with retrieval, re-ranking, and generation components.\n",
    "\n",
    "## Technical Constraints\n",
    "- **Python 3.10+** compatible code throughout\n",
    "- **Open-source models only** for embeddings and re-ranking (sentence-transformers, cross-encoders)\n",
    "- **OpenAI SDK** used only for the final generation step\n",
    "- All dependencies installable via pip\n",
    "- Deterministic behavior with fixed random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\n",
      "‚úÖ Python version compatible\n",
      "‚úÖ numpy available\n",
      "‚úÖ pandas available\n",
      "‚úÖ scikit-learn available\n",
      "‚úÖ rank_bm25 available\n",
      "‚ùå sentence-transformers not found\n",
      "‚úÖ torch available\n",
      "‚úÖ faiss-cpu available\n",
      "‚úÖ tqdm available\n",
      "‚úÖ openai available\n",
      "\n",
      "üì¶ Installing 1 missing packages...\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "‚úÖ Successfully installed sentence-transformers\n",
      "\n",
      "üîÑ Importing all packages...\n",
      "‚úÖ All imports successful!\n",
      "\n",
      "üéØ Random seeds set for reproducibility\n",
      "üöÄ Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Setup & Environment Check\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "# Print Python version to verify compatibility\n",
    "print(f\"Python version: {sys.version}\")\n",
    "if sys.version_info < (3, 10):\n",
    "    print(\"‚ö†Ô∏è  Warning: This notebook requires Python 3.10 or higher\")\n",
    "else:\n",
    "    print(\"‚úÖ Python version compatible\")\n",
    "\n",
    "# Define required packages\n",
    "required_packages = [\n",
    "    'numpy',\n",
    "    'pandas', \n",
    "    'scikit-learn',\n",
    "    'rank_bm25',\n",
    "    'sentence-transformers',\n",
    "    'torch',\n",
    "    'faiss-cpu',\n",
    "    'tqdm',\n",
    "    'openai'\n",
    "]\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip programmatically\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
    "        print(f\"‚úÖ Successfully installed {package_name}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"‚ùå Failed to install {package_name}\")\n",
    "        return False\n",
    "\n",
    "def check_and_install_packages(packages):\n",
    "    \"\"\"Check if packages are available, install if missing\"\"\"\n",
    "    missing_packages = []\n",
    "    \n",
    "    # First pass: check what's missing\n",
    "    for package in packages:\n",
    "        # Handle special cases for import names vs package names\n",
    "        import_name = package\n",
    "        if package == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        elif package == 'faiss-cpu':\n",
    "            import_name = 'faiss'\n",
    "        elif package == 'rank_bm25':\n",
    "            import_name = 'rank_bm25'\n",
    "            \n",
    "        spec = importlib.util.find_spec(import_name)\n",
    "        if spec is None:\n",
    "            missing_packages.append(package)\n",
    "            print(f\"‚ùå {package} not found\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {package} available\")\n",
    "    \n",
    "    # Second pass: install missing packages\n",
    "    if missing_packages:\n",
    "        print(f\"\\nüì¶ Installing {len(missing_packages)} missing packages...\")\n",
    "        for package in missing_packages:\n",
    "            install_package(package)\n",
    "    \n",
    "    return missing_packages\n",
    "\n",
    "# Check and install packages\n",
    "missing = check_and_install_packages(required_packages)\n",
    "\n",
    "print(\"\\nüîÑ Importing all packages...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "    import torch\n",
    "    try:\n",
    "        import faiss\n",
    "        FAISS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        FAISS_AVAILABLE = False\n",
    "        print(\"‚ÑπÔ∏è  FAISS not available, will use sklearn NearestNeighbors\")\n",
    "    from tqdm import tqdm\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please restart the kernel and try again.\")\n",
    "\n",
    "# Set global random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "print(\"\\nüéØ Random seeds set for reproducibility\")\n",
    "print(\"üöÄ Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines information retrieval with text generation to create more accurate, grounded responses. Instead of relying solely on a language model's training data, RAG first retrieves relevant documents from a knowledge base, then uses those documents to generate answers.\n",
    "\n",
    "### Core Components\n",
    "1. **Indexing**: Preprocessing and storing documents for efficient retrieval\n",
    "2. **Retrieval**: Finding relevant documents given a query\n",
    "3. **Fusion**: Combining results from multiple retrieval methods\n",
    "4. **Re-ranking**: Refining the order of retrieved documents\n",
    "5. **Generation**: Creating answers using retrieved context\n",
    "\n",
    "### Key Benefits\n",
    "- **Reduces hallucinations** by grounding responses in actual documents\n",
    "- **Enables up-to-date information** without retraining models\n",
    "- **Provides citations** for transparency and verification\n",
    "- **Scales efficiently** to large knowledge bases\n",
    "\n",
    "### Trade-offs\n",
    "RAG adds complexity and latency but dramatically improves factual accuracy and allows dynamic knowledge updates. The retrieval quality directly impacts the final answer quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Quality retrieval starts with quality data. Clean, well-structured documents are essential for effective RAG systems. Each document should have consistent fields and clear, focused content.\n",
    "\n",
    "### Key Principles\n",
    "- **Structured format**: Use consistent fields (id, title, text) for easy processing\n",
    "- **Appropriate granularity**: Documents should be focused but comprehensive\n",
    "- **Clean text**: Remove formatting artifacts, normalize whitespace\n",
    "- **Diverse content**: Include varied topics to test retrieval robustness\n",
    "\n",
    "### Our Synthetic Corpus\n",
    "We'll create a small but diverse dataset spanning multiple domains (astronomy, cooking, programming, history, health, sports). This allows us to test different retrieval methods on varied content types. Including some near-duplicates and paraphrases helps evaluate robustness to semantic similarity.\n",
    "\n",
    "### Licensing Note\n",
    "When using real data, always verify licensing terms and respect copyright. Our synthetic dataset avoids these concerns while providing realistic testing scenarios.\n",
    "\n",
    "**Deterministic seeds** ensure reproducible results across runs, crucial for comparing retrieval methods fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Created synthetic corpus with 31 documents\n",
      "üìÇ Domains covered: 6 unique prefixes\n",
      "üìè Text length range: 313 - 385 characters\n",
      "\n",
      "üìà Documents per domain:\n",
      "  Python/Programming: 6 documents\n",
      "  Astronomy: 5 documents\n",
      "  Cooking: 5 documents\n",
      "  History: 5 documents\n",
      "  Health: 5 documents\n",
      "  Sports: 5 documents\n",
      "\n",
      "üíæ Corpus saved to ./data/corpus.csv\n",
      "\n",
      "üìã Sample documents:\n",
      "\n",
      "[ast_001] Black Holes: Cosmic Vacuum Cleaners\n",
      "Text preview: Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can es...\n",
      "\n",
      "[ast_002] The Life Cycle of Stars\n",
      "Text preview: Stars begin as clouds of gas and dust that collapse under gravity. Nuclear fusion ignites in their c...\n",
      "\n",
      "[ast_003] Exoplanets: Worlds Beyond Our Solar System\n",
      "Text preview: Exoplanets are planets orbiting stars other than our Sun. Over 5,000 have been discovered using meth...\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive synthetic dataset creation\n",
    "# This dataset will be used throughout the entire notebook\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Define synthetic documents across 6 domains with varied content types\n",
    "# Each domain has 5-8 documents including some near-duplicates for robustness testing\n",
    "documents = [\n",
    "    # Astronomy domain\n",
    "    {\"id\": \"ast_001\", \"title\": \"Black Holes: Cosmic Vacuum Cleaners\", \n",
    "     \"text\": \"Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can escape once it crosses the event horizon. They form when massive stars collapse at the end of their lives. The event horizon is the boundary beyond which escape becomes impossible. Supermassive black holes, found at galaxy centers, can have masses millions of times greater than our Sun.\"},\n",
    "    \n",
    "    {\"id\": \"ast_002\", \"title\": \"The Life Cycle of Stars\", \n",
    "     \"text\": \"Stars begin as clouds of gas and dust that collapse under gravity. Nuclear fusion ignites in their cores, converting hydrogen to helium and releasing enormous energy. Main sequence stars like our Sun burn steadily for billions of years. Massive stars burn through their fuel quickly and end in spectacular supernovae, while smaller stars become white dwarfs.\"},\n",
    "    \n",
    "    {\"id\": \"ast_003\", \"title\": \"Exoplanets: Worlds Beyond Our Solar System\", \n",
    "     \"text\": \"Exoplanets are planets orbiting stars other than our Sun. Over 5,000 have been discovered using methods like transit photometry and radial velocity measurements. Some exoplanets orbit in their star's habitable zone where liquid water could exist. The Kepler Space Telescope revolutionized exoplanet discovery by monitoring stellar brightness changes.\"},\n",
    "    \n",
    "    {\"id\": \"ast_004\", \"title\": \"The Expanding Universe and Dark Energy\", \n",
    "     \"text\": \"The universe is expanding, with distant galaxies moving away from us at speeds proportional to their distance. This expansion is accelerating due to dark energy, a mysterious force comprising about 68% of the universe. Dark matter, another enigma, makes up 27%, leaving only 5% as ordinary matter we can see and touch.\"},\n",
    "    \n",
    "    {\"id\": \"ast_005\", \"title\": \"Stellar Collapse and Black Hole Formation\", \n",
    "     \"text\": \"When massive stars exhaust their nuclear fuel, they can no longer support themselves against gravity. The core collapses in milliseconds, creating conditions so extreme that a black hole forms. The collapsing matter crushes down to infinite density at the singularity, while the event horizon marks the point of no return for any approaching matter or light.\"},\n",
    "    \n",
    "    # Cooking domain  \n",
    "    {\"id\": \"cook_001\", \"title\": \"The Maillard Reaction: Science of Browning\", \n",
    "     \"text\": \"The Maillard reaction occurs when proteins and sugars are heated together, creating complex flavors and browning in foods. This reaction is responsible for the golden crust on bread, the sear on steaks, and the rich flavor of roasted coffee. Temperature control is crucial - too low and the reaction won't occur, too high and food burns before proper flavor development.\"},\n",
    "    \n",
    "    {\"id\": \"cook_002\", \"title\": \"Knife Skills: Foundation of Good Cooking\", \n",
    "     \"text\": \"Proper knife technique is essential for efficient cooking. A sharp knife is safer than a dull one because it requires less pressure and provides better control. The basic cuts include julienne, dice, chiffonade, and brunoise. Consistent sizing ensures even cooking. Always curl your fingers and use a rocking motion with the knife tip staying on the cutting board.\"},\n",
    "    \n",
    "    {\"id\": \"cook_003\", \"title\": \"Understanding Heat Transfer in Cooking\", \n",
    "     \"text\": \"Heat transfer in cooking occurs through conduction, convection, and radiation. Conduction happens when food touches a hot surface like a pan. Convection involves hot air or liquid circulating around food. Radiation transfers heat through electromagnetic waves, like in broiling. Understanding these principles helps choose the right cooking method for desired results.\"},\n",
    "    \n",
    "    {\"id\": \"cook_004\", \"title\": \"Fermentation: Ancient Preservation Technique\", \n",
    "     \"text\": \"Fermentation uses beneficial microorganisms to preserve food and create unique flavors. Lactic acid fermentation produces sauerkraut and kimchi, while alcoholic fermentation creates wine and beer. The process requires controlling temperature, pH, and salt levels to favor good bacteria over harmful ones. Fermented foods are also rich in probiotics.\"},\n",
    "    \n",
    "    {\"id\": \"cook_005\", \"title\": \"Browning and Caramelization in Food\", \n",
    "     \"text\": \"Browning reactions transform food appearance and flavor through heat. Caramelization breaks down sugars at high temperatures, creating sweet, complex flavors. The Maillard reaction between amino acids and sugars produces savory, nutty notes. Both reactions require precise temperature control to achieve desired results without burning.\"},\n",
    "    \n",
    "    # Python/Programming domain\n",
    "    {\"id\": \"py_001\", \"title\": \"Python List Comprehensions: Elegant Iteration\", \n",
    "     \"text\": \"List comprehensions provide a concise way to create lists in Python. They're more readable and often faster than traditional for loops. The syntax is [expression for item in iterable if condition]. For example, [x**2 for x in range(10) if x%2==0] creates squares of even numbers. Nested comprehensions are possible but should be used sparingly for readability.\"},\n",
    "    \n",
    "    {\"id\": \"py_002\", \"title\": \"Python Decorators: Modifying Function Behavior\", \n",
    "     \"text\": \"Decorators are a powerful Python feature that allows modifying or extending function behavior without changing the function itself. They use the @decorator_name syntax above function definitions. Common uses include logging, timing, authentication, and caching. Decorators are functions that take functions as arguments and return modified functions.\"},\n",
    "    \n",
    "    {\"id\": \"py_003\", \"title\": \"Virtual Environments: Dependency Management\", \n",
    "     \"text\": \"Virtual environments create isolated Python installations for different projects, preventing dependency conflicts. Tools like venv, conda, and pipenv help manage environments. Each environment has its own Python interpreter and package installations. This isolation ensures that project dependencies don't interfere with each other or the system Python.\"},\n",
    "    \n",
    "    {\"id\": \"py_004\", \"title\": \"Exception Handling: Graceful Error Management\", \n",
    "     \"text\": \"Python's try-except blocks handle errors gracefully without crashing programs. Different exception types can be caught specifically, and finally blocks ensure cleanup code runs regardless of errors. Raising custom exceptions helps create more informative error messages. Proper exception handling makes code more robust and user-friendly.\"},\n",
    "    \n",
    "    {\"id\": \"py_005\", \"title\": \"Python Generators: Memory-Efficient Iteration\", \n",
    "     \"text\": \"Generators create iterators that yield values on-demand rather than storing them in memory. They use the yield keyword instead of return. Generator expressions use parentheses instead of square brackets like list comprehensions. This lazy evaluation saves memory for large datasets and enables infinite sequences.\"},\n",
    "    \n",
    "    {\"id\": \"py_006\", \"title\": \"List Processing in Python\", \n",
    "     \"text\": \"Python offers multiple ways to process lists efficiently. List comprehensions create new lists with concise syntax: [x*2 for x in numbers]. The map() function applies operations to all elements, while filter() selects elements meeting criteria. These functional approaches are often more readable than traditional loops and can be combined for complex transformations.\"},\n",
    "    \n",
    "    # History domain\n",
    "    {\"id\": \"hist_001\", \"title\": \"The Fall of Constantinople 1453\", \n",
    "     \"text\": \"The fall of Constantinople in 1453 marked the end of the Byzantine Empire and the beginning of Ottoman dominance in southeastern Europe. Sultan Mehmed II's forces used massive cannons to breach the city's ancient walls after a 53-day siege. This event closed the eastern trade routes, spurring European exploration of new paths to Asia.\"},\n",
    "    \n",
    "    {\"id\": \"hist_002\", \"title\": \"The Printing Press Revolution\", \n",
    "     \"text\": \"Johannes Gutenberg's printing press, invented around 1440, revolutionized the spread of information. It enabled mass production of books, making knowledge accessible beyond the wealthy elite. The printing press facilitated the Protestant Reformation, scientific revolution, and Renaissance by allowing rapid dissemination of ideas across Europe.\"},\n",
    "    \n",
    "    {\"id\": \"hist_003\", \"title\": \"The Silk Road: Ancient Trade Networks\", \n",
    "     \"text\": \"The Silk Road was a network of trade routes connecting East and West from ancient times through the medieval period. It facilitated exchange of goods, ideas, and cultures between China, Central Asia, and Europe. Merchants traded silk, spices, precious metals, and technologies, while also spreading religions and knowledge across continents.\"},\n",
    "    \n",
    "    {\"id\": \"hist_004\", \"title\": \"The Industrial Revolution's Impact\", \n",
    "     \"text\": \"The Industrial Revolution transformed society from agricultural to manufacturing-based economies. Steam engines, textile mills, and factory systems changed how people lived and worked. Urbanization accelerated as people moved to cities for factory jobs. This period laid the foundation for modern capitalism and dramatically increased production capacity.\"},\n",
    "    \n",
    "    {\"id\": \"hist_005\", \"title\": \"Medieval Trade Routes and Commerce\", \n",
    "     \"text\": \"Medieval trade networks connected distant regions through overland and maritime routes. The Silk Road linked Asia and Europe, while Mediterranean trade connected Africa, Asia, and Europe. Merchants established trade guilds for protection and standardization. These routes facilitated not only commerce but also cultural and technological exchange between civilizations.\"},\n",
    "    \n",
    "    # Health domain\n",
    "    {\"id\": \"health_001\", \"title\": \"The Immune System: Body's Defense Network\", \n",
    "     \"text\": \"The immune system protects the body from pathogens through innate and adaptive responses. White blood cells, antibodies, and specialized organs work together to identify and eliminate threats. Vaccines train the immune system to recognize specific pathogens without causing illness. A healthy lifestyle supports immune function through proper nutrition, exercise, and sleep.\"},\n",
    "    \n",
    "    {\"id\": \"health_002\", \"title\": \"Cardiovascular Health and Exercise\", \n",
    "     \"text\": \"Regular exercise strengthens the heart muscle and improves circulation throughout the body. Aerobic activities like walking, swimming, and cycling reduce blood pressure and cholesterol levels. Exercise also increases HDL (good) cholesterol while lowering LDL (bad) cholesterol. The American Heart Association recommends 150 minutes of moderate exercise weekly.\"},\n",
    "    \n",
    "    {\"id\": \"health_003\", \"title\": \"Nutrition: Fuel for the Human Body\", \n",
    "     \"text\": \"Proper nutrition provides energy and essential nutrients for optimal body function. Macronutrients include carbohydrates for energy, proteins for tissue repair, and fats for hormone production. Micronutrients like vitamins and minerals support various biochemical processes. A balanced diet includes diverse foods from all food groups in appropriate proportions.\"},\n",
    "    \n",
    "    {\"id\": \"health_004\", \"title\": \"Sleep: The Foundation of Health\", \n",
    "     \"text\": \"Sleep is crucial for physical and mental health, allowing the body to repair and consolidate memories. Adults need 7-9 hours of quality sleep nightly. During sleep, the brain clears metabolic waste and strengthens neural connections. Sleep deprivation impairs immune function, cognitive performance, and emotional regulation.\"},\n",
    "    \n",
    "    {\"id\": \"health_005\", \"title\": \"Mental Health and Well-being\", \n",
    "     \"text\": \"Mental health encompasses emotional, psychological, and social well-being. It affects how we think, feel, and act. Factors like genetics, brain chemistry, trauma, and life experiences influence mental health. Regular exercise, social connections, stress management, and professional support when needed all contribute to maintaining good mental health.\"},\n",
    "    \n",
    "    # Sports domain\n",
    "    {\"id\": \"sport_001\", \"title\": \"Biomechanics of Athletic Performance\", \n",
    "     \"text\": \"Biomechanics analyzes human movement to optimize athletic performance and prevent injuries. It studies forces, motion, and energy transfer during sports activities. Understanding lever systems, joint angles, and muscle activation patterns helps athletes improve technique. Video analysis and force plates provide detailed biomechanical data for performance enhancement.\"},\n",
    "    \n",
    "    {\"id\": \"sport_002\", \"title\": \"Sports Psychology: Mental Game\", \n",
    "     \"text\": \"Sports psychology focuses on the mental aspects of athletic performance. Techniques like visualization, goal setting, and mindfulness help athletes manage pressure and maintain focus. Mental training is as important as physical preparation. Confidence, motivation, and concentration significantly impact performance outcomes in competitive sports.\"},\n",
    "    \n",
    "    {\"id\": \"sport_003\", \"title\": \"Recovery and Regeneration in Athletics\", \n",
    "     \"text\": \"Recovery is essential for athletic adaptation and performance improvement. It includes active recovery, proper nutrition, hydration, and sleep. Techniques like ice baths, compression therapy, and massage aid muscle recovery. Periodized training incorporates rest phases to prevent overtraining and reduce injury risk while maximizing performance gains.\"},\n",
    "    \n",
    "    {\"id\": \"sport_004\", \"title\": \"Strength Training Principles\", \n",
    "     \"text\": \"Effective strength training follows progressive overload, gradually increasing resistance to stimulate adaptation. Compound exercises like squats and deadlifts work multiple muscle groups efficiently. Training frequency, volume, and intensity must be balanced for optimal results. Proper form prevents injury and ensures targeted muscle activation during resistance exercises.\"},\n",
    "    \n",
    "    {\"id\": \"sport_005\", \"title\": \"Endurance Training Methodologies\", \n",
    "     \"text\": \"Endurance training improves the body's ability to sustain prolonged physical activity. Training zones based on heart rate or power output optimize different energy systems. Base training builds aerobic capacity, while high-intensity intervals improve lactate threshold. Periodization varies training stress to promote adaptation while preventing burnout and overtraining syndrome.\"}\n",
    "]\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "corpus_df = pd.DataFrame(documents)\n",
    "\n",
    "print(f\"üìä Created synthetic corpus with {len(corpus_df)} documents\")\n",
    "print(f\"üìÇ Domains covered: {len(corpus_df['id'].str[:4].unique())} unique prefixes\")\n",
    "print(f\"üìè Text length range: {corpus_df['text'].str.len().min()} - {corpus_df['text'].str.len().max()} characters\")\n",
    "\n",
    "# Show distribution by domain\n",
    "domain_counts = corpus_df['id'].str[:4].value_counts()\n",
    "print(\"\\nüìà Documents per domain:\")\n",
    "domain_names = {\n",
    "    'ast_': 'Astronomy',\n",
    "    'cook': 'Cooking', \n",
    "    'py_0': 'Python/Programming',\n",
    "    'hist': 'History',\n",
    "    'heal': 'Health',\n",
    "    'spor': 'Sports'\n",
    "}\n",
    "for prefix, count in domain_counts.items():\n",
    "    domain_name = domain_names.get(prefix, prefix)\n",
    "    print(f\"  {domain_name}: {count} documents\")\n",
    "\n",
    "# Save to CSV for reuse throughout the notebook\n",
    "corpus_df.to_csv('./data/corpus.csv', index=False)\n",
    "print(\"\\nüíæ Corpus saved to ./data/corpus.csv\")\n",
    "\n",
    "# Display sample documents\n",
    "print(\"\\nüìã Sample documents:\")\n",
    "for i in range(3):\n",
    "    doc = corpus_df.iloc[i]\n",
    "    print(f\"\\n[{doc['id']}] {doc['title']}\")\n",
    "    print(f\"Text preview: {doc['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**TF-IDF** is a fundamental text retrieval technique that scores documents based on term importance. It combines two concepts:\n",
    "- **Term Frequency (TF)**: How often a term appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How rare a term is across the entire corpus\n",
    "\n",
    "### Intuition\n",
    "Words that appear frequently in a document but rarely across the corpus are most important for distinguishing that document. Common words like \"the\" and \"and\" get low scores, while specific terms get higher scores.\n",
    "\n",
    "### How It Works\n",
    "Documents are converted to vectors where each dimension represents a unique term's TF-IDF score. Query similarity is computed using cosine similarity between the query vector and document vectors.\n",
    "\n",
    "### Advantages\n",
    "- **Fast and interpretable**: Clear scoring rationale\n",
    "- **No training required**: Works immediately on any corpus\n",
    "- **Memory efficient**: Sparse vectors for large vocabularies\n",
    "\n",
    "### Limitations\n",
    "- **Vocabulary mismatch**: Can't match synonyms (\"car\" vs \"automobile\")\n",
    "- **Word order ignored**: \"dog bites man\" = \"man bites dog\"\n",
    "- **No semantic understanding**: Relies purely on exact word matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Building TF-IDF matrix...\n",
      "üìä TF-IDF matrix shape: (31, 1734)\n",
      "üìù Vocabulary size: 1734\n",
      "üíæ Matrix sparsity: 0.0% zeros\n",
      "\n",
      "üîç Testing TF-IDF retrieval:\n",
      "\n",
      "üìã Query: 'black holes and event horizons'\n",
      "Top 5 results:\n",
      "  1. [ast_001] Black Holes: Cosmic Vacuum Cleaners (score: 0.445)\n",
      "      Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can es...\n",
      "  2. [ast_005] Stellar Collapse and Black Hole Formation (score: 0.103)\n",
      "      When massive stars exhaust their nuclear fuel, they can no longer support themselves against gravity...\n",
      "  3. [hist_001] The Fall of Constantinople 1453 (score: 0.043)\n",
      "      The fall of Constantinople in 1453 marked the end of the Byzantine Empire and the beginning of Ottom...\n",
      "  4. [py_005] Python Generators: Memory-Efficient Iteration (score: 0.000)\n",
      "      Generators create iterators that yield values on-demand rather than storing them in memory. They use...\n",
      "  5. [ast_002] The Life Cycle of Stars (score: 0.000)\n",
      "      Stars begin as clouds of gas and dust that collapse under gravity. Nuclear fusion ignites in their c...\n",
      "\n",
      "üìã Query: 'python programming decorators'\n",
      "Top 5 results:\n",
      "  1. [py_002] Python Decorators: Modifying Function Behavior (score: 0.250)\n",
      "      Decorators are a powerful Python feature that allows modifying or extending function behavior withou...\n",
      "  2. [py_003] Virtual Environments: Dependency Management (score: 0.154)\n",
      "      Virtual environments create isolated Python installations for different projects, preventing depende...\n",
      "  3. [py_001] Python List Comprehensions: Elegant Iteration (score: 0.056)\n",
      "      List comprehensions provide a concise way to create lists in Python. They're more readable and often...\n",
      "  4. [py_006] List Processing in Python (score: 0.052)\n",
      "      Python offers multiple ways to process lists efficiently. List comprehensions create new lists with ...\n",
      "  5. [py_004] Exception Handling: Graceful Error Management (score: 0.046)\n",
      "      Python's try-except blocks handle errors gracefully without crashing programs. Different exception t...\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF retrieval implementation using scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize TF-IDF vectorizer with optimized parameters\n",
    "# - Use both unigrams and bigrams to capture phrases like \"black holes\"\n",
    "# - Convert to lowercase for normalization\n",
    "# - Remove English stop words to focus on meaningful terms\n",
    "# - Set max_features to control vocabulary size and computation\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Include both single words and word pairs\n",
    "    lowercase=True,      # Normalize case\n",
    "    stop_words='english', # Remove common words like 'the', 'and'\n",
    "    max_features=10000,  # Limit vocabulary size for efficiency\n",
    "    min_df=1,            # Include terms that appear in at least 1 document\n",
    "    max_df=0.8           # Exclude terms that appear in >80% of documents\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform texts to TF-IDF vectors\n",
    "print(\"üîÑ Building TF-IDF matrix...\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_df['text'])\n",
    "\n",
    "print(f\"üìä TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"üìù Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"üíæ Matrix sparsity: {(1 - tfidf_matrix.nnz / tfidf_matrix.size) * 100:.1f}% zeros\")\n",
    "\n",
    "def query_tfidf(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve documents using TF-IDF similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        top_k (int): Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: Tuples of (document_index, similarity_score, document_info)\n",
    "    \"\"\"\n",
    "    # Transform query using the same vectorizer fitted on corpus\n",
    "    query_vector = tfidf_vectorizer.transform([query_text])\n",
    "    \n",
    "    # Compute cosine similarity between query and all documents\n",
    "    # Cosine similarity ranges from 0 (no similarity) to 1 (identical)\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get indices of top-k most similar documents\n",
    "    top_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Build results with document info and scores\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_info = {\n",
    "            'id': corpus_df.iloc[idx]['id'],\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['text']\n",
    "        }\n",
    "        results.append((idx, similarity_scores[idx], doc_info))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test TF-IDF retrieval with example queries\n",
    "test_queries = [\n",
    "    \"black holes and event horizons\",\n",
    "    \"python programming decorators\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing TF-IDF retrieval:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìã Query: '{query}'\")\n",
    "    results = query_tfidf(query, top_k=5)\n",
    "    \n",
    "    print(\"Top 5 results:\")\n",
    "    for rank, (idx, score, doc_info) in enumerate(results, 1):\n",
    "        print(f\"  {rank}. [{doc_info['id']}] {doc_info['title']} (score: {score:.3f})\")\n",
    "        # Show first 100 characters of text as snippet\n",
    "        snippet = doc_info['text'][:100] + '...' if len(doc_info['text']) > 100 else doc_info['text']\n",
    "        print(f\"      {snippet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25: Best Matching 25\n",
    "\n",
    "**BM25** is a probabilistic ranking function that often outperforms TF-IDF for information retrieval. It improves upon TF-IDF by addressing two key limitations:\n",
    "\n",
    "### Key Improvements\n",
    "1. **Term Saturation**: TF-IDF scores increase linearly with term frequency, but BM25 uses a saturation function. After a certain point, additional occurrences contribute less to the score.\n",
    "\n",
    "2. **Document Length Normalization**: BM25 adjusts for document length, preventing longer documents from having unfair advantages simply due to more term occurrences.\n",
    "\n",
    "### Parameters\n",
    "- **k1** (typically 1.2-2.0): Controls term frequency saturation\n",
    "- **b** (typically 0.75): Controls document length normalization strength\n",
    "\n",
    "### When BM25 Excels\n",
    "BM25 typically outperforms TF-IDF for keyword search, especially with:\n",
    "- Varied document lengths\n",
    "- Collections where term frequency patterns matter\n",
    "- Traditional information retrieval tasks\n",
    "\n",
    "Both TF-IDF and BM25 are **lexical** methods‚Äîthey rely on exact word matches and don't understand semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenizing corpus for BM25...\n",
      "üìä BM25 index built for 31 documents\n",
      "üìù Average document length: 50.1 tokens\n",
      "\n",
      "üîç Comparing BM25 vs TF-IDF retrieval:\n",
      "\n",
      "üìã Query: 'black holes event horizon'\n",
      "\n",
      "üèÜ BM25 Top 3:\n",
      "  1. [ast_001] Black Holes: Cosmic Vacuum Cleaners (BM25: 13.18)\n",
      "  2. [ast_005] Stellar Collapse and Black Hole Formation (BM25: 6.68)\n",
      "  3. [hist_001] The Fall of Constantinople 1453 (BM25: 1.98)\n",
      "\n",
      "üìä TF-IDF Top 3:\n",
      "  1. [ast_001] Black Holes: Cosmic Vacuum Cleaners (TF-IDF: 0.537)\n",
      "  2. [ast_005] Stellar Collapse and Black Hole Formation (TF-IDF: 0.179)\n",
      "  3. [hist_001] The Fall of Constantinople 1453 (TF-IDF: 0.036)\n",
      "\n",
      "üîó Overlap: 3/3 documents match between methods\n",
      "\n",
      "üìã Query: 'python decorators function behavior'\n",
      "\n",
      "üèÜ BM25 Top 3:\n",
      "  1. [py_002] Python Decorators: Modifying Function Behavior (BM25: 11.77)\n",
      "  2. [py_006] List Processing in Python (BM25: 3.07)\n",
      "  3. [py_003] Virtual Environments: Dependency Management (BM25: 2.66)\n",
      "\n",
      "üìä TF-IDF Top 3:\n",
      "  1. [py_002] Python Decorators: Modifying Function Behavior (TF-IDF: 0.368)\n",
      "  2. [py_003] Virtual Environments: Dependency Management (TF-IDF: 0.094)\n",
      "  3. [py_006] List Processing in Python (TF-IDF: 0.063)\n",
      "\n",
      "üîó Overlap: 3/3 documents match between methods\n",
      "\n",
      "üìã Query: 'exercise cardiovascular health'\n",
      "\n",
      "üèÜ BM25 Top 3:\n",
      "  1. [health_005] Mental Health and Well-being (BM25: 6.34)\n",
      "  2. [health_002] Cardiovascular Health and Exercise (BM25: 3.55)\n",
      "  3. [health_004] Sleep: The Foundation of Health (BM25: 2.56)\n",
      "\n",
      "üìä TF-IDF Top 3:\n",
      "  1. [health_005] Mental Health and Well-being (TF-IDF: 0.284)\n",
      "  2. [health_002] Cardiovascular Health and Exercise (TF-IDF: 0.186)\n",
      "  3. [health_004] Sleep: The Foundation of Health (TF-IDF: 0.080)\n",
      "\n",
      "üîó Overlap: 3/3 documents match between methods\n"
     ]
    }
   ],
   "source": [
    "# BM25 retrieval implementation using rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Simple tokenizer for BM25 preprocessing.\n",
    "    Converts to lowercase, removes punctuation, and splits on whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tokens\n",
    "    \"\"\"\n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regex (keeps alphanumeric and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Split on whitespace and filter empty strings\n",
    "    tokens = [token for token in text.split() if token]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Tokenize all documents in the corpus for BM25\n",
    "print(\"üîÑ Tokenizing corpus for BM25...\")\n",
    "tokenized_corpus = [simple_tokenizer(doc_text) for doc_text in corpus_df['text']]\n",
    "\n",
    "# Initialize BM25 with default parameters (k1=1.2, b=0.75)\n",
    "# These are well-tested values that work well across many domains\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "print(f\"üìä BM25 index built for {len(tokenized_corpus)} documents\")\n",
    "print(f\"üìù Average document length: {np.mean([len(doc) for doc in tokenized_corpus]):.1f} tokens\")\n",
    "\n",
    "def query_bm25(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve documents using BM25 scoring.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        top_k (int): Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: Tuples of (document_index, bm25_score, document_info)\n",
    "    \"\"\"\n",
    "    # Tokenize query using same tokenizer as corpus\n",
    "    query_tokens = simple_tokenizer(query_text)\n",
    "    \n",
    "    # Get BM25 scores for all documents\n",
    "    # Higher scores indicate better matches\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get indices of top-k highest scoring documents\n",
    "    top_indices = np.argsort(bm25_scores)[-top_k:][::-1]\n",
    "    \n",
    "    # Build results with document info and scores\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_info = {\n",
    "            'id': corpus_df.iloc[idx]['id'],\n",
    "            'title': corpus_df.iloc[idx]['title'],\n",
    "            'text': corpus_df.iloc[idx]['text']\n",
    "        }\n",
    "        results.append((idx, bm25_scores[idx], doc_info))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare BM25 vs TF-IDF on the same queries\n",
    "comparison_queries = [\n",
    "    \"black holes event horizon\",\n",
    "    \"python decorators function behavior\",\n",
    "    \"exercise cardiovascular health\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Comparing BM25 vs TF-IDF retrieval:\")\n",
    "for query in comparison_queries:\n",
    "    print(f\"\\nüìã Query: '{query}'\")\n",
    "    \n",
    "    # Get results from both methods\n",
    "    bm25_results = query_bm25(query, top_k=3)\n",
    "    tfidf_results = query_tfidf(query, top_k=3)\n",
    "    \n",
    "    print(\"\\nüèÜ BM25 Top 3:\")\n",
    "    for rank, (idx, score, doc_info) in enumerate(bm25_results, 1):\n",
    "        print(f\"  {rank}. [{doc_info['id']}] {doc_info['title']} (BM25: {score:.2f})\")\n",
    "    \n",
    "    print(\"\\nüìä TF-IDF Top 3:\")\n",
    "    for rank, (idx, score, doc_info) in enumerate(tfidf_results, 1):\n",
    "        print(f\"  {rank}. [{doc_info['id']}] {doc_info['title']} (TF-IDF: {score:.3f})\")\n",
    "    \n",
    "    # Show overlap between methods\n",
    "    bm25_ids = {doc_info['id'] for _, _, doc_info in bm25_results}\n",
    "    tfidf_ids = {doc_info['id'] for _, _, doc_info in tfidf_results}\n",
    "    overlap = bm25_ids.intersection(tfidf_ids)\n",
    "    print(f\"\\nüîó Overlap: {len(overlap)}/3 documents match between methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Semantic Vector Representations\n",
    "\n",
    "**Embeddings** represent text as dense vectors in high-dimensional space where semantically similar texts are close together. Unlike lexical methods (TF-IDF, BM25), embeddings can match concepts even with different vocabulary.\n",
    "\n",
    "### Key Advantages\n",
    "- **Semantic understanding**: Matches \"car\" with \"automobile\"\n",
    "- **Cross-lingual capability**: Can work across languages\n",
    "- **Context awareness**: Considers word relationships and context\n",
    "\n",
    "### Vector Similarity\n",
    "Cosine similarity measures the angle between vectors, ranging from -1 to 1. Values closer to 1 indicate higher semantic similarity.\n",
    "\n",
    "### Trade-offs\n",
    "- **Computational cost**: Embedding models require more resources\n",
    "- **Model dependence**: Quality depends on training data and architecture\n",
    "- **Interpretability**: Harder to understand why documents match\n",
    "\n",
    "### Approximate Nearest Neighbors (ANN)\n",
    "For large corpora, exact similarity search becomes slow. ANN algorithms like FAISS provide fast approximate search with minimal accuracy loss.\n",
    "\n",
    "**Privacy note**: Using local models keeps data on your machine, unlike API-based embedding services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating chunked corpus...\n",
      "üìä Created 31 chunks from 31 documents\n",
      "üìè Average chunk length: 49.4 words\n",
      "üìà Chunk length distribution:\n",
      "  Min: 43 words\n",
      "  Max: 64 words\n",
      "  Median: 48.0 words\n",
      "\n",
      "üìã Example chunking for 'Black Holes: Cosmic Vacuum Cleaners':\n",
      "Original text (64 words):\n",
      "  Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can escape once it crosses the event horizon. They form ...\n",
      "\n",
      "Chunks created: 1\n",
      "  Chunk 1 (64 words): Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can es...\n"
     ]
    }
   ],
   "source": [
    "# Text chunking for better embedding performance\n",
    "# Chunking breaks long documents into smaller, focused pieces\n",
    "# This improves embedding quality and allows more precise retrieval\n",
    "\n",
    "def chunk_text(text, chunk_size_words=180, overlap_words=30):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks of specified word count.\n",
    "    Overlap helps maintain context across chunk boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size_words (int): Target words per chunk\n",
    "        overlap_words (int): Words to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    # If text is shorter than chunk size, return as single chunk\n",
    "    if len(words) <= chunk_size_words:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        # Define chunk end, ensuring we don't exceed word count\n",
    "        end = min(start + chunk_size_words, len(words))\n",
    "        \n",
    "        # Extract chunk and join words back to text\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = ' '.join(chunk_words)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move start position, accounting for overlap\n",
    "        # If this is the last chunk, break to avoid infinite loop\n",
    "        if end >= len(words):\n",
    "            break\n",
    "        start = end - overlap_words\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunked corpus for better embedding performance\n",
    "print(\"üîÑ Creating chunked corpus...\")\n",
    "chunked_data = []\n",
    "\n",
    "for _, row in corpus_df.iterrows():\n",
    "    doc_chunks = chunk_text(row['text'], chunk_size_words=180, overlap_words=30)\n",
    "    \n",
    "    for chunk_idx, text_chunk in enumerate(doc_chunks):\n",
    "        chunked_data.append({\n",
    "            'doc_id': row['id'],\n",
    "            'chunk_id': f\"{row['id']}_chunk_{chunk_idx}\",\n",
    "            'title': row['title'],\n",
    "            'chunk_text': text_chunk\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easy manipulation\n",
    "chunked_corpus = pd.DataFrame(chunked_data)\n",
    "\n",
    "print(f\"üìä Created {len(chunked_corpus)} chunks from {len(corpus_df)} documents\")\n",
    "print(f\"üìè Average chunk length: {chunked_corpus['chunk_text'].str.split().str.len().mean():.1f} words\")\n",
    "\n",
    "# Show chunk length distribution\n",
    "chunk_lengths = chunked_corpus['chunk_text'].str.split().str.len()\n",
    "print(f\"üìà Chunk length distribution:\")\n",
    "print(f\"  Min: {chunk_lengths.min()} words\")\n",
    "print(f\"  Max: {chunk_lengths.max()} words\")\n",
    "print(f\"  Median: {chunk_lengths.median():.1f} words\")\n",
    "\n",
    "# Show example of chunking\n",
    "sample_doc = corpus_df.iloc[0]\n",
    "sample_chunks = chunk_text(sample_doc['text'])\n",
    "print(f\"\\nüìã Example chunking for '{sample_doc['title']}':\")\n",
    "print(f\"Original text ({len(sample_doc['text'].split())} words):\")\n",
    "print(f\"  {sample_doc['text'][:150]}...\")\n",
    "print(f\"\\nChunks created: {len(sample_chunks)}\")\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    print(f\"  Chunk {i+1} ({len(chunk.split())} words): {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üìê Model produces 384-dimensional vectors\n",
      "üîÑ Computing embeddings for 31 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e59c9c3b2c442b7a2fad9bc6baa0377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 31 embeddings to ./data/embeddings.npz\n",
      "üìä Embedding matrix shape: (31, 384)\n",
      "üéØ Embeddings normalized for cosine similarity\n",
      "üöÄ Using FAISS for fast similarity search\n",
      "üíæ FAISS index saved to ./data/faiss.index\n",
      "\n",
      "üîç Testing semantic search:\n",
      "\n",
      "üìã Query: 'stellar collapse and gravitational effects'\n",
      "Top 5 semantic matches:\n",
      "  1. [ast_005] Stellar Collapse and Black Hole Formation (similarity: 0.595)\n",
      "      When massive stars exhaust their nuclear fuel, they can no longer support themselves against gravity...\n",
      "  2. [ast_002] The Life Cycle of Stars (similarity: 0.498)\n",
      "      Stars begin as clouds of gas and dust that collapse under gravity. Nuclear fusion ignites in their c...\n",
      "  3. [ast_001] Black Holes: Cosmic Vacuum Cleaners (similarity: 0.376)\n",
      "      Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can es...\n",
      "  4. [ast_004] The Expanding Universe and Dark Energy (similarity: 0.252)\n",
      "      The universe is expanding, with distant galaxies moving away from us at speeds proportional to their...\n",
      "  5. [hist_001] The Fall of Constantinople 1453 (similarity: 0.227)\n",
      "      The fall of Constantinople in 1453 marked the end of the Byzantine Empire and the beginning of Ottom...\n",
      "\n",
      "üìã Query: 'modifying function behavior in programming'\n",
      "Top 5 semantic matches:\n",
      "  1. [py_002] Python Decorators: Modifying Function Behavior (similarity: 0.451)\n",
      "      Decorators are a powerful Python feature that allows modifying or extending function behavior withou...\n",
      "  2. [py_006] List Processing in Python (similarity: 0.312)\n",
      "      Python offers multiple ways to process lists efficiently. List comprehensions create new lists with ...\n",
      "  3. [py_004] Exception Handling: Graceful Error Management (similarity: 0.267)\n",
      "      Python's try-except blocks handle errors gracefully without crashing programs. Different exception t...\n",
      "  4. [py_005] Python Generators: Memory-Efficient Iteration (similarity: 0.263)\n",
      "      Generators create iterators that yield values on-demand rather than storing them in memory. They use...\n",
      "  5. [py_001] Python List Comprehensions: Elegant Iteration (similarity: 0.199)\n",
      "      List comprehensions provide a concise way to create lists in Python. They're more readable and often...\n",
      "\n",
      "üìã Query: 'heart health and physical activity'\n",
      "Top 5 semantic matches:\n",
      "  1. [health_002] Cardiovascular Health and Exercise (similarity: 0.630)\n",
      "      Regular exercise strengthens the heart muscle and improves circulation throughout the body. Aerobic ...\n",
      "  2. [health_005] Mental Health and Well-being (similarity: 0.358)\n",
      "      Mental health encompasses emotional, psychological, and social well-being. It affects how we think, ...\n",
      "  3. [sport_005] Endurance Training Methodologies (similarity: 0.339)\n",
      "      Endurance training improves the body's ability to sustain prolonged physical activity. Training zone...\n",
      "  4. [sport_002] Sports Psychology: Mental Game (similarity: 0.307)\n",
      "      Sports psychology focuses on the mental aspects of athletic performance. Techniques like visualizati...\n",
      "  5. [sport_001] Biomechanics of Athletic Performance (similarity: 0.290)\n",
      "      Biomechanics analyzes human movement to optimize athletic performance and prevent injuries. It studi...\n"
     ]
    }
   ],
   "source": [
    "# Semantic embeddings using open-source sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# Use a high-quality, lightweight open-source embedding model\n",
    "# all-MiniLM-L6-v2 provides good performance with reasonable speed\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# Alternative models (commented for reference):\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"  # Better quality, slightly larger\n",
    "# model_name = \"intfloat/e5-small-v2\"     # Good multilingual support\n",
    "\n",
    "print(f\"ü§ñ Loading embedding model: {model_name}\")\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"üìê Model produces {embedding_model.get_sentence_embedding_dimension()}-dimensional vectors\")\n",
    "\n",
    "# Check if embeddings already exist to avoid recomputation\n",
    "embeddings_file = './data/embeddings.npz'\n",
    "chunks_file = './data/chunked_corpus.csv'\n",
    "\n",
    "if os.path.exists(embeddings_file) and os.path.exists(chunks_file):\n",
    "    print(\"üìÇ Loading pre-computed embeddings...\")\n",
    "    embeddings_data = np.load(embeddings_file)\n",
    "    chunk_embeddings = embeddings_data['embeddings']\n",
    "    chunked_corpus = pd.read_csv(chunks_file)\n",
    "    print(f\"‚úÖ Loaded {len(chunk_embeddings)} embeddings from cache\")\n",
    "else:\n",
    "    # Compute embeddings for all chunks with progress bar\n",
    "    print(f\"üîÑ Computing embeddings for {len(chunked_corpus)} chunks...\")\n",
    "    \n",
    "    # Use tqdm for progress tracking during embedding computation\n",
    "    chunk_texts = chunked_corpus['chunk_text'].tolist()\n",
    "    \n",
    "    # sentence-transformers handles batching internally for efficiency\n",
    "    chunk_embeddings = embedding_model.encode(\n",
    "        chunk_texts, \n",
    "        batch_size=32,          # Process in batches for memory efficiency\n",
    "        show_progress_bar=True, # Show progress during computation\n",
    "        convert_to_numpy=True   # Return as numpy array\n",
    "    )\n",
    "    \n",
    "    # Save embeddings and chunks for future use\n",
    "    np.savez_compressed(embeddings_file, embeddings=chunk_embeddings)\n",
    "    chunked_corpus.to_csv(chunks_file, index=False)\n",
    "    print(f\"üíæ Saved {len(chunk_embeddings)} embeddings to {embeddings_file}\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity using dot product\n",
    "# This makes cosine similarity equivalent to dot product, which is faster\n",
    "from sklearn.preprocessing import normalize\n",
    "chunk_embeddings_normalized = normalize(chunk_embeddings, norm='l2')\n",
    "\n",
    "print(f\"üìä Embedding matrix shape: {chunk_embeddings.shape}\")\n",
    "print(f\"üéØ Embeddings normalized for cosine similarity\")\n",
    "\n",
    "# Choose between FAISS and sklearn based on availability\n",
    "if FAISS_AVAILABLE:\n",
    "    # Use FAISS for fast approximate nearest neighbor search\n",
    "    print(\"üöÄ Using FAISS for fast similarity search\")\n",
    "    \n",
    "    # Create FAISS index for inner product (equivalent to cosine with normalized vectors)\n",
    "    embedding_dim = chunk_embeddings_normalized.shape[1]\n",
    "    faiss_index = faiss.IndexFlatIP(embedding_dim)  # Inner Product index\n",
    "    \n",
    "    # Add embeddings to index\n",
    "    faiss_index.add(chunk_embeddings_normalized.astype(np.float32))\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss_index_file = './data/faiss.index'\n",
    "    faiss.write_index(faiss_index, faiss_index_file)\n",
    "    print(f\"üíæ FAISS index saved to {faiss_index_file}\")\n",
    "    \n",
    "    search_backend = 'faiss'\n",
    "    \n",
    "else:\n",
    "    # Use sklearn NearestNeighbors as fallback\n",
    "    print(\"üìö Using sklearn NearestNeighbors for similarity search\")\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    # Create sklearn nearest neighbors index\n",
    "    nn_index = NearestNeighbors(\n",
    "        n_neighbors=20,      # Maximum neighbors to consider\n",
    "        metric='cosine',     # Use cosine similarity\n",
    "        algorithm='brute'    # Exact search for small datasets\n",
    "    )\n",
    "    nn_index.fit(chunk_embeddings)\n",
    "    \n",
    "    search_backend = 'sklearn'\n",
    "\n",
    "def embed_query(query_text):\n",
    "    \"\"\"\n",
    "    Convert query text to embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Query embedding vector\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query_text], convert_to_numpy=True)\n",
    "    return normalize(query_embedding, norm='l2')[0]  # Normalize and return single vector\n",
    "\n",
    "def semantic_search(query_text, top_k=10):\n",
    "    \"\"\"\n",
    "    Search for semantically similar chunks using embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        top_k (int): Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: Tuples of (chunk_index, similarity_score, chunk_info)\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = embed_query(query_text)\n",
    "    \n",
    "    if search_backend == 'faiss':\n",
    "        # Use FAISS for fast search\n",
    "        scores, indices = faiss_index.search(\n",
    "            query_embedding.reshape(1, -1).astype(np.float32), \n",
    "            top_k\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "            chunk_info = {\n",
    "                'chunk_id': chunked_corpus.iloc[idx]['chunk_id'],\n",
    "                'doc_id': chunked_corpus.iloc[idx]['doc_id'],\n",
    "                'title': chunked_corpus.iloc[idx]['title'],\n",
    "                'chunk_text': chunked_corpus.iloc[idx]['chunk_text']\n",
    "            }\n",
    "            results.append((idx, score, chunk_info))\n",
    "    \n",
    "    else:\n",
    "        # Use sklearn for search\n",
    "        distances, indices = nn_index.kneighbors(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            n_neighbors=min(top_k, len(chunked_corpus))\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "            # Convert cosine distance to similarity (1 - distance)\n",
    "            similarity = 1 - distance\n",
    "            chunk_info = {\n",
    "                'chunk_id': chunked_corpus.iloc[idx]['chunk_id'],\n",
    "                'doc_id': chunked_corpus.iloc[idx]['doc_id'],\n",
    "                'title': chunked_corpus.iloc[idx]['title'],\n",
    "                'chunk_text': chunked_corpus.iloc[idx]['chunk_text']\n",
    "            }\n",
    "            results.append((idx, similarity, chunk_info))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "test_queries = [\n",
    "    \"stellar collapse and gravitational effects\",\n",
    "    \"modifying function behavior in programming\",\n",
    "    \"heart health and physical activity\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing semantic search:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìã Query: '{query}'\")\n",
    "    results = semantic_search(query, top_k=5)\n",
    "    \n",
    "    print(\"Top 5 semantic matches:\")\n",
    "    for rank, (idx, score, chunk_info) in enumerate(results, 1):\n",
    "        print(f\"  {rank}. [{chunk_info['doc_id']}] {chunk_info['title']} (similarity: {score:.3f})\")\n",
    "        snippet = chunk_info['chunk_text'][:100] + '...'\n",
    "        print(f\"      {snippet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retrieval: Best of Both Worlds\n",
    "\n",
    "**Hybrid retrieval** combines lexical (BM25) and semantic (embeddings) approaches to leverage their complementary strengths:\n",
    "\n",
    "### Lexical Strengths\n",
    "- Exact term matching for technical terms and proper names\n",
    "- Fast computation and interpretable results\n",
    "- Robust to domain shifts\n",
    "\n",
    "### Semantic Strengths\n",
    "- Conceptual matching beyond exact words\n",
    "- Better handling of synonyms and paraphrases\n",
    "- Context-aware understanding\n",
    "\n",
    "### Hybrid Strategy\n",
    "1. **Union approach**: Get candidates from both methods\n",
    "2. **Score normalization**: Make scores comparable across methods\n",
    "3. **Rank fusion**: Combine rankings intelligently\n",
    "\n",
    "### Score Normalization\n",
    "Different retrieval methods produce scores on different scales. Min-max normalization maps all scores to [0,1] range:\n",
    "```\n",
    "normalized_score = (score - min_score) / (max_score - min_score)\n",
    "```\n",
    "\n",
    "This ensures fair combination across methods, preventing one method from dominating due to larger score magnitudes.\n",
    "\n",
    "Hybrid retrieval typically improves both **recall** (finding relevant documents) and **robustness** (handling diverse query types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing hybrid retrieval for: 'black hole formation from stellar collapse'\n",
      "\n",
      "üìä Found 15 unique documents from hybrid approach\n",
      "\n",
      "üìà Method coverage:\n",
      "  BM25 found: 10 documents\n",
      "  Semantic found: 10 documents\n",
      "  Both methods found: 5 documents\n",
      "  Union: 15 documents\n",
      "\n",
      "üìã Top 5 candidates for rank fusion:\n",
      "ID\tTitle\tBM25_norm\tSem_norm\tBM25_rank\tSem_rank\n",
      "--------------------------------------------------------------------------------\n",
      "ast_001\tBlack Holes: Cosmic Vacuum Cle...\t1.00\t\t0.82\t\t1\t\t2\n",
      "ast_005\tStellar Collapse and Black Hol...\t0.96\t\t1.00\t\t2\t\t2\n",
      "ast_003\tExoplanets: Worlds Beyond Our ...\t0.56\t\t0.21\t\t3\t\t2\n",
      "ast_002\tThe Life Cycle of Stars...\t0.43\t\t0.78\t\t4\t\t2\n",
      "health_003\tNutrition: Fuel for the Human ...\t0.30\t\t0.00\t\t5\t\t--\n"
     ]
    }
   ],
   "source": [
    "# Hybrid retrieval combining BM25 and semantic embeddings\n",
    "# This approach leverages both lexical and semantic matching\n",
    "\n",
    "def normalize_scores(scores):\n",
    "    \"\"\"\n",
    "    Normalize scores to [0, 1] range using min-max normalization.\n",
    "    Handles edge case where all scores are identical.\n",
    "    \n",
    "    Args:\n",
    "        scores (list): List of numerical scores\n",
    "    \n",
    "    Returns:\n",
    "        list: Normalized scores in [0, 1] range\n",
    "    \"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "    \n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    \n",
    "    # Handle case where all scores are identical\n",
    "    if max_score == min_score:\n",
    "        return [1.0] * len(scores)  # Give all equal high score\n",
    "    \n",
    "    # Apply min-max normalization\n",
    "    normalized = [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "    return normalized\n",
    "\n",
    "def hybrid_retrieve(query_text, top_k_lex=15, top_k_sem=15):\n",
    "    \"\"\"\n",
    "    Combine lexical (BM25) and semantic retrieval results.\n",
    "    Uses union of candidates and normalizes scores for fair comparison.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        top_k_lex (int): Number of lexical results to retrieve\n",
    "        top_k_sem (int): Number of semantic results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results with normalized scores\n",
    "    \"\"\"\n",
    "    # Get BM25 results on original documents (not chunks)\n",
    "    bm25_results = query_bm25(query_text, top_k=top_k_lex)\n",
    "    \n",
    "    # Get semantic results on chunks\n",
    "    semantic_results = semantic_search(query_text, top_k=top_k_sem)\n",
    "    \n",
    "    # Create unified candidate list\n",
    "    candidates = {}\n",
    "    \n",
    "    # Process BM25 results\n",
    "    bm25_scores = [score for _, score, _ in bm25_results]\n",
    "    normalized_bm25_scores = normalize_scores(bm25_scores)\n",
    "    \n",
    "    for (doc_idx, score, doc_info), norm_score in zip(bm25_results, normalized_bm25_scores):\n",
    "        doc_id = doc_info['id']\n",
    "        if doc_id not in candidates:\n",
    "            candidates[doc_id] = {\n",
    "                'doc_id': doc_id,\n",
    "                'title': doc_info['title'],\n",
    "                'text': doc_info['text'],\n",
    "                'bm25_score': score,\n",
    "                'bm25_rank': len(candidates) + 1,\n",
    "                'bm25_normalized': norm_score,\n",
    "                'semantic_score': 0,\n",
    "                'semantic_rank': None,\n",
    "                'semantic_normalized': 0\n",
    "            }\n",
    "    \n",
    "    # Process semantic results\n",
    "    semantic_scores = [score for _, score, _ in semantic_results]\n",
    "    normalized_semantic_scores = normalize_scores(semantic_scores)\n",
    "    \n",
    "    for (chunk_idx, score, chunk_info), norm_score in zip(semantic_results, normalized_semantic_scores):\n",
    "        doc_id = chunk_info['doc_id']\n",
    "        \n",
    "        if doc_id not in candidates:\n",
    "            # Find original document info for new semantic candidates\n",
    "            orig_doc = corpus_df[corpus_df['id'] == doc_id].iloc[0]\n",
    "            candidates[doc_id] = {\n",
    "                'doc_id': doc_id,\n",
    "                'title': orig_doc['title'],\n",
    "                'text': orig_doc['text'],\n",
    "                'bm25_score': 0,\n",
    "                'bm25_rank': None,\n",
    "                'bm25_normalized': 0,\n",
    "                'semantic_score': score,\n",
    "                'semantic_rank': len([r for r in semantic_results if r[2]['doc_id'] == doc_id]) + 1,\n",
    "                'semantic_normalized': norm_score\n",
    "            }\n",
    "        else:\n",
    "            # Update existing candidate with semantic info\n",
    "            # Take best semantic score if multiple chunks from same document\n",
    "            if score > candidates[doc_id]['semantic_score']:\n",
    "                candidates[doc_id]['semantic_score'] = score\n",
    "                candidates[doc_id]['semantic_normalized'] = norm_score\n",
    "                candidates[doc_id]['semantic_rank'] = len([r for r in semantic_results if r[2]['doc_id'] == doc_id]) + 1\n",
    "    \n",
    "    # Convert to DataFrame for easy manipulation\n",
    "    hybrid_results = pd.DataFrame.from_dict(candidates, orient='index')\n",
    "    \n",
    "    return hybrid_results\n",
    "\n",
    "# Test hybrid retrieval\n",
    "test_query = \"black hole formation from stellar collapse\"\n",
    "\n",
    "print(f\"üîç Testing hybrid retrieval for: '{test_query}'\")\n",
    "hybrid_df = hybrid_retrieve(test_query, top_k_lex=10, top_k_sem=10)\n",
    "\n",
    "print(f\"\\nüìä Found {len(hybrid_df)} unique documents from hybrid approach\")\n",
    "\n",
    "# Show method comparison\n",
    "bm25_only = hybrid_df[hybrid_df['bm25_rank'].notna()].shape[0]\n",
    "semantic_only = hybrid_df[hybrid_df['semantic_rank'].notna()].shape[0]\n",
    "both_methods = hybrid_df[(hybrid_df['bm25_rank'].notna()) & (hybrid_df['semantic_rank'].notna())].shape[0]\n",
    "\n",
    "print(f\"\\nüìà Method coverage:\")\n",
    "print(f\"  BM25 found: {bm25_only} documents\")\n",
    "print(f\"  Semantic found: {semantic_only} documents\")\n",
    "print(f\"  Both methods found: {both_methods} documents\")\n",
    "print(f\"  Union: {len(hybrid_df)} documents\")\n",
    "\n",
    "# Display top hybrid results sorted by combined score\n",
    "print(f\"\\nüìã Top 5 candidates for rank fusion:\")\n",
    "print(\"ID\\tTitle\\tBM25_norm\\tSem_norm\\tBM25_rank\\tSem_rank\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in hybrid_df.head().iterrows():\n",
    "    bm25_rank = f\"{row['bm25_rank']:.0f}\" if pd.notna(row['bm25_rank']) else \"--\"\n",
    "    sem_rank = f\"{row['semantic_rank']:.0f}\" if pd.notna(row['semantic_rank']) else \"--\"\n",
    "    \n",
    "    print(f\"{row['doc_id']}\\t{row['title'][:30]}...\\t\"\n",
    "          f\"{row['bm25_normalized']:.2f}\\t\\t{row['semantic_normalized']:.2f}\\t\\t\"\n",
    "          f\"{bm25_rank}\\t\\t{sem_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Fusion: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF)** is a simple yet effective method for combining rankings from multiple retrieval systems. It's particularly robust because it relies on ranks rather than raw scores.\n",
    "\n",
    "### RRF Formula\n",
    "For each document, RRF computes:\n",
    "```\n",
    "RRF_score = Œ£ (1 / (k + rank_i))\n",
    "```\n",
    "where:\n",
    "- `rank_i` is the document's rank in system i\n",
    "- `k` is a constant (typically 60) that controls the contribution curve\n",
    "- The sum is over all systems that retrieved the document\n",
    "\n",
    "### Why RRF Works Well\n",
    "1. **Rank-based**: Avoids issues with different score scales\n",
    "2. **Robust**: Less sensitive to outliers than score-based fusion\n",
    "3. **Simple**: No parameter tuning beyond choosing k\n",
    "4. **Proven**: Works well across different retrieval types\n",
    "\n",
    "### Parameter k\n",
    "- **k=60** (default): Balanced contribution from all ranks\n",
    "- **Lower k**: Top ranks dominate more\n",
    "- **Higher k**: More uniform contribution across ranks\n",
    "\n",
    "Documents appearing in multiple systems get higher RRF scores, while high-ranking documents in any single system also score well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Applying RRF to query: 'black hole formation from stellar collapse'\n",
      "\n",
      "üìä RRF Results (k=60):\n",
      "\n",
      "Top 10 documents after RRF fusion:\n",
      "Rank\tDoc ID\tTitle\t\t\tRRF Score\tBM25 Rank\tSem Rank\n",
      "-------------------------------------------------------------------------------------\n",
      "1\tast_001\tBlack Holes: Cosmic Vacuu...\t0.0325\t\t1\t\t2\n",
      "2\tast_005\tStellar Collapse and Blac...\t0.0323\t\t2\t\t2\n",
      "3\tast_003\tExoplanets: Worlds Beyond...\t0.0320\t\t3\t\t2\n",
      "4\tast_002\tThe Life Cycle of Stars     \t0.0318\t\t4\t\t2\n",
      "5\tast_004\tThe Expanding Universe an...\t0.0306\t\t9\t\t2\n",
      "6\tcook_005\tBrowning and Caramelizati...\t0.0161\t\t--\t\t2\n",
      "7\tcook_002\tKnife Skills: Foundation ...\t0.0161\t\t--\t\t2\n",
      "8\tcook_001\tThe Maillard Reaction: Sc...\t0.0161\t\t--\t\t2\n",
      "9\thist_001\tThe Fall of Constantinopl...\t0.0161\t\t--\t\t2\n",
      "10\thist_002\tThe Printing Press Revolu...\t0.0161\t\t--\t\t2\n",
      "\n",
      "üìà Method contribution analysis for top 5 results:\n",
      "\n",
      "1. [ast_001] Black Holes: Cosmic Vacuum Cleaners\n",
      "   Total RRF score: 0.0325\n",
      "   BM25: rank 1 ‚Üí contribution 0.0164\n",
      "   Semantic: rank 2 ‚Üí contribution 0.0161\n",
      "\n",
      "2. [ast_005] Stellar Collapse and Black Hole Formation\n",
      "   Total RRF score: 0.0323\n",
      "   BM25: rank 2 ‚Üí contribution 0.0161\n",
      "   Semantic: rank 2 ‚Üí contribution 0.0161\n",
      "\n",
      "3. [ast_003] Exoplanets: Worlds Beyond Our Solar System\n",
      "   Total RRF score: 0.0320\n",
      "   BM25: rank 3 ‚Üí contribution 0.0159\n",
      "   Semantic: rank 2 ‚Üí contribution 0.0161\n",
      "\n",
      "4. [ast_002] The Life Cycle of Stars\n",
      "   Total RRF score: 0.0318\n",
      "   BM25: rank 4 ‚Üí contribution 0.0156\n",
      "   Semantic: rank 2 ‚Üí contribution 0.0161\n",
      "\n",
      "5. [ast_004] The Expanding Universe and Dark Energy\n",
      "   Total RRF score: 0.0306\n",
      "   BM25: rank 9 ‚Üí contribution 0.0145\n",
      "   Semantic: rank 2 ‚Üí contribution 0.0161\n",
      "\n",
      "üî¨ Effect of different k values on top result:\n",
      "k= 10: [ast_001] Black Holes: Cosmic Vacuum Cleaners... (RRF: 0.1742)\n",
      "k= 30: [ast_001] Black Holes: Cosmic Vacuum Cleaners... (RRF: 0.0635)\n",
      "k= 60: [ast_001] Black Holes: Cosmic Vacuum Cleaners... (RRF: 0.0325)\n",
      "k=100: [ast_001] Black Holes: Cosmic Vacuum Cleaners... (RRF: 0.0197)\n"
     ]
    }
   ],
   "source": [
    "# Reciprocal Rank Fusion (RRF) implementation\n",
    "# RRF is a robust method for combining rankings from different retrieval systems\n",
    "\n",
    "def rrf_fuse(rankings, k=60):\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion to combine multiple ranking methods.\n",
    "    \n",
    "    Args:\n",
    "        rankings (dict): Dictionary mapping method names to lists of (doc_id, rank) tuples\n",
    "        k (int): RRF parameter controlling rank contribution curve (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "        list: Tuples of (doc_id, rrf_score, method_details)\n",
    "    \"\"\"\n",
    "    # Collect all unique document IDs\n",
    "    all_doc_ids = set()\n",
    "    for method_rankings in rankings.values():\n",
    "        all_doc_ids.update(doc_id for doc_id, _ in method_rankings)\n",
    "    \n",
    "    # Calculate RRF scores for each document\n",
    "    rrf_scores = {}\n",
    "    method_details = {}\n",
    "    \n",
    "    for doc_id in all_doc_ids:\n",
    "        total_score = 0\n",
    "        doc_method_info = {}\n",
    "        \n",
    "        # Sum reciprocal ranks across all methods that retrieved this document\n",
    "        for method_name, method_rankings in rankings.items():\n",
    "            # Find this document's rank in the current method\n",
    "            doc_rank = None\n",
    "            for d_id, rank in method_rankings:\n",
    "                if d_id == doc_id:\n",
    "                    doc_rank = rank\n",
    "                    break\n",
    "            \n",
    "            if doc_rank is not None:\n",
    "                # Calculate reciprocal rank contribution\n",
    "                contribution = 1 / (k + doc_rank)\n",
    "                total_score += contribution\n",
    "                doc_method_info[method_name] = {\n",
    "                    'rank': doc_rank,\n",
    "                    'contribution': contribution\n",
    "                }\n",
    "            else:\n",
    "                doc_method_info[method_name] = {\n",
    "                    'rank': None,\n",
    "                    'contribution': 0\n",
    "                }\n",
    "        \n",
    "        rrf_scores[doc_id] = total_score\n",
    "        method_details[doc_id] = doc_method_info\n",
    "    \n",
    "    # Sort by RRF score (highest first)\n",
    "    sorted_results = sorted(\n",
    "        [(doc_id, score, method_details[doc_id]) \n",
    "         for doc_id, score in rrf_scores.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "def apply_rrf_to_hybrid(hybrid_df, k=60):\n",
    "    \"\"\"\n",
    "    Apply RRF to hybrid retrieval results.\n",
    "    \n",
    "    Args:\n",
    "        hybrid_df (pd.DataFrame): Hybrid retrieval results\n",
    "        k (int): RRF parameter\n",
    "    \n",
    "    Returns:\n",
    "        list: RRF fused results\n",
    "    \"\"\"\n",
    "    # Prepare rankings for RRF\n",
    "    rankings = {}\n",
    "    \n",
    "    # BM25 rankings (only for documents that have BM25 results)\n",
    "    bm25_docs = hybrid_df[hybrid_df['bm25_rank'].notna()]\n",
    "    if len(bm25_docs) > 0:\n",
    "        bm25_rankings = [(row['doc_id'], row['bm25_rank']) \n",
    "                        for _, row in bm25_docs.iterrows()]\n",
    "        rankings['BM25'] = bm25_rankings\n",
    "    \n",
    "    # Semantic rankings (only for documents that have semantic results)\n",
    "    semantic_docs = hybrid_df[hybrid_df['semantic_rank'].notna()]\n",
    "    if len(semantic_docs) > 0:\n",
    "        semantic_rankings = [(row['doc_id'], row['semantic_rank']) \n",
    "                           for _, row in semantic_docs.iterrows()]\n",
    "        rankings['Semantic'] = semantic_rankings\n",
    "    \n",
    "    # Apply RRF\n",
    "    rrf_results = rrf_fuse(rankings, k=k)\n",
    "    \n",
    "    # Enrich results with document information\n",
    "    enriched_results = []\n",
    "    for doc_id, rrf_score, method_info in rrf_results:\n",
    "        doc_row = hybrid_df[hybrid_df['doc_id'] == doc_id].iloc[0]\n",
    "        enriched_results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'title': doc_row['title'],\n",
    "            'text': doc_row['text'],\n",
    "            'rrf_score': rrf_score,\n",
    "            'method_info': method_info,\n",
    "            'bm25_rank': doc_row['bm25_rank'] if pd.notna(doc_row['bm25_rank']) else None,\n",
    "            'semantic_rank': doc_row['semantic_rank'] if pd.notna(doc_row['semantic_rank']) else None\n",
    "        })\n",
    "    \n",
    "    return enriched_results\n",
    "\n",
    "# Test RRF on our hybrid results\n",
    "test_query = \"black hole formation from stellar collapse\"\n",
    "print(f\"üîç Applying RRF to query: '{test_query}'\")\n",
    "\n",
    "# Get hybrid results\n",
    "hybrid_df = hybrid_retrieve(test_query, top_k_lex=10, top_k_sem=10)\n",
    "\n",
    "# Apply RRF\n",
    "rrf_results = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "\n",
    "print(f\"\\nüìä RRF Results (k=60):\")\n",
    "print(\"\\nTop 10 documents after RRF fusion:\")\n",
    "print(\"Rank\\tDoc ID\\tTitle\\t\\t\\tRRF Score\\tBM25 Rank\\tSem Rank\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for rank, result in enumerate(rrf_results[:10], 1):\n",
    "    title_short = result['title'][:25] + '...' if len(result['title']) > 25 else result['title']\n",
    "    bm25_rank = f\"{result['bm25_rank']:.0f}\" if result['bm25_rank'] is not None else \"--\"\n",
    "    sem_rank = f\"{result['semantic_rank']:.0f}\" if result['semantic_rank'] is not None else \"--\"\n",
    "    \n",
    "    print(f\"{rank}\\t{result['doc_id']}\\t{title_short:<28}\\t{result['rrf_score']:.4f}\\t\\t{bm25_rank}\\t\\t{sem_rank}\")\n",
    "\n",
    "# Analyze method contributions\n",
    "print(f\"\\nüìà Method contribution analysis for top 5 results:\")\n",
    "for i, result in enumerate(rrf_results[:5], 1):\n",
    "    print(f\"\\n{i}. [{result['doc_id']}] {result['title']}\")\n",
    "    print(f\"   Total RRF score: {result['rrf_score']:.4f}\")\n",
    "    \n",
    "    for method, info in result['method_info'].items():\n",
    "        if info['rank'] is not None:\n",
    "            print(f\"   {method}: rank {info['rank']:.0f} ‚Üí contribution {info['contribution']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   {method}: not found ‚Üí contribution 0.0000\")\n",
    "\n",
    "# Test different k values to show effect\n",
    "print(f\"\\nüî¨ Effect of different k values on top result:\")\n",
    "for k_val in [10, 30, 60, 100]:\n",
    "    rrf_k = apply_rrf_to_hybrid(hybrid_df, k=k_val)\n",
    "    top_result = rrf_k[0]\n",
    "    print(f\"k={k_val:3d}: [{top_result['doc_id']}] {top_result['title'][:40]}... (RRF: {top_result['rrf_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking with Cross-Encoders\n",
    "\n",
    "**Cross-encoders** are transformer models that score (query, passage) pairs directly, providing more accurate relevance estimates than individual embeddings. They're the \"second stage\" in a two-stage retrieval pipeline.\n",
    "\n",
    "### How Cross-Encoders Work\n",
    "Unlike bi-encoders (like sentence-transformers) that encode query and document separately, cross-encoders:\n",
    "1. Concatenate query and passage as input: `[CLS] query [SEP] passage [SEP]`\n",
    "2. Use full attention across query-passage pairs\n",
    "3. Output a single relevance score\n",
    "\n",
    "### Advantages\n",
    "- **Higher accuracy**: Full attention between query and passage\n",
    "- **Better ranking**: Specifically trained for relevance scoring\n",
    "- **Fine-grained scoring**: Can distinguish subtle relevance differences\n",
    "\n",
    "### Trade-offs\n",
    "- **Computational cost**: Must process each (query, candidate) pair\n",
    "- **Latency**: Slower than embedding-based similarity\n",
    "- **Scale limitations**: Practical only for re-ranking small candidate sets\n",
    "\n",
    "### Best Practice\n",
    "Use cross-encoders to re-rank the top-N (typically 10-50) candidates from faster retrieval methods. This gives you both speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading cross-encoder: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b7cfbae0ad4576937bd7623adf8553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a98a6e4ed24d5a9dd30e865f59ef8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651a59a858c2430bae7c0b8ebceaf17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd8edec096e4776aca60aa50819c0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85dcb765f324704a3e747eb3aaa0495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c3c456f0bf479d8f096cbb2111ee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cross-encoder loaded successfully\n",
      "\n",
      "üîç Testing cross-encoder re-ranking for: 'black hole formation from stellar collapse'\n",
      "\n",
      "üìä Re-ranking top 20 RRF candidates\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.33 seconds\n",
      "\n",
      "üìà Comparison: RRF vs Cross-Encoder ranking\n",
      "\n",
      "RRF Ranking (before re-ranking):\n",
      "Rank\tDoc ID\tTitle\t\t\t\tRRF Score\n",
      "----------------------------------------------------------------------\n",
      "1\tast_001\tBlack Holes: Cosmic Vacuum Cleaners   \t0.0325\n",
      "2\tast_005\tStellar Collapse and Black Hole For...\t0.0323\n",
      "3\tast_003\tExoplanets: Worlds Beyond Our Solar...\t0.0320\n",
      "4\tast_002\tThe Life Cycle of Stars               \t0.0318\n",
      "5\thist_004\tThe Industrial Revolution's Impact    \t0.0313\n",
      "\n",
      "Cross-Encoder Ranking (after re-ranking):\n",
      "Rank\tDoc ID\tTitle\t\t\t\tCE Score\n",
      "----------------------------------------------------------------------\n",
      "1\tast_005\tStellar Collapse and Black Hole For...\t8.8854\n",
      "2\tast_001\tBlack Holes: Cosmic Vacuum Cleaners   \t4.3416\n",
      "3\tast_002\tThe Life Cycle of Stars               \t-8.0985\n",
      "4\thealth_003\tNutrition: Fuel for the Human Body    \t-11.2308\n",
      "5\thealth_001\tThe Immune System: Body's Defense N...\t-11.2742\n",
      "\n",
      "üìã Top 5 results with snippets:\n",
      "\n",
      "1. [ast_005] Stellar Collapse and Black Hole Formation\n",
      "   Cross-encoder score: 8.8854\n",
      "   Original RRF score: 0.0323\n",
      "   Snippet: When massive stars exhaust their nuclear fuel, they can no longer support themselves against gravity. The core collapses in milliseconds, creating conditions so extreme that a black hole forms. The co...\n",
      "\n",
      "2. [ast_001] Black Holes: Cosmic Vacuum Cleaners\n",
      "   Cross-encoder score: 4.3416\n",
      "   Original RRF score: 0.0325\n",
      "   Snippet: Black holes are regions of spacetime where gravity is so strong that nothing, not even light, can escape once it crosses the event horizon. They form when massive stars collapse at the end of their li...\n",
      "\n",
      "3. [ast_002] The Life Cycle of Stars\n",
      "   Cross-encoder score: -8.0985\n",
      "   Original RRF score: 0.0318\n",
      "   Snippet: Stars begin as clouds of gas and dust that collapse under gravity. Nuclear fusion ignites in their cores, converting hydrogen to helium and releasing enormous energy. Main sequence stars like our Sun ...\n",
      "\n",
      "4. [health_003] Nutrition: Fuel for the Human Body\n",
      "   Cross-encoder score: -11.2308\n",
      "   Original RRF score: 0.0154\n",
      "   Snippet: Proper nutrition provides energy and essential nutrients for optimal body function. Macronutrients include carbohydrates for energy, proteins for tissue repair, and fats for hormone production. Micron...\n",
      "\n",
      "5. [health_001] The Immune System: Body's Defense Network\n",
      "   Cross-encoder score: -11.2742\n",
      "   Original RRF score: 0.0147\n",
      "   Snippet: The immune system protects the body from pathogens through innate and adaptive responses. White blood cells, antibodies, and specialized organs work together to identify and eliminate threats. Vaccine...\n",
      "\n",
      "üîÑ Ranking changes analysis:\n",
      "Biggest ranking changes:\n",
      "  [health_003] ‚Üë 11 positions: Nutrition: Fuel for the Human Body...\n",
      "  [health_001] ‚Üë 11 positions: The Immune System: Body's Defense Network...\n",
      "  [hist_002] ‚Üë 7 positions: The Printing Press Revolution...\n",
      "  [hist_003] ‚Üì 2 positions: The Silk Road: Ancient Trade Networks...\n",
      "  [ast_005] ‚Üë 1 positions: Stellar Collapse and Black Hole Formation...\n"
     ]
    }
   ],
   "source": [
    "# Re-ranking with open-source cross-encoder models\n",
    "# Cross-encoders provide more accurate relevance scoring for final ranking\n",
    "import time\n",
    "\n",
    "# Load a lightweight cross-encoder model\n",
    "# ms-marco-MiniLM is trained on Microsoft's passage ranking dataset\n",
    "cross_encoder_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# Alternative models (commented for reference):\n",
    "# cross_encoder_model_name = \"BAAI/bge-reranker-base\"          # Higher quality, larger\n",
    "# cross_encoder_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"  # Faster, smaller\n",
    "\n",
    "print(f\"ü§ñ Loading cross-encoder: {cross_encoder_model_name}\")\n",
    "cross_encoder = CrossEncoder(cross_encoder_model_name)\n",
    "print(\"‚úÖ Cross-encoder loaded successfully\")\n",
    "\n",
    "def rerank_with_cross_encoder(query_text, candidates, top_k=10):\n",
    "    \"\"\"\n",
    "    Re-rank candidates using a cross-encoder model.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        candidates (list): List of candidate documents with text\n",
    "        top_k (int): Number of top results to return after re-ranking\n",
    "    \n",
    "    Returns:\n",
    "        list: Re-ranked candidates with cross-encoder scores\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare (query, passage) pairs for the cross-encoder\n",
    "    # Use document title + text for better context\n",
    "    query_passage_pairs = []\n",
    "    for candidate in candidates:\n",
    "        # Combine title and text for richer passage representation\n",
    "        passage_text = f\"{candidate['title']}. {candidate['text']}\"\n",
    "        query_passage_pairs.append([query_text, passage_text])\n",
    "    \n",
    "    # Get relevance scores from cross-encoder\n",
    "    # Scores are logits that can be interpreted as relevance strength\n",
    "    print(f\"üîÑ Computing cross-encoder scores for {len(query_passage_pairs)} candidates...\")\n",
    "    relevance_scores = cross_encoder.predict(query_passage_pairs)\n",
    "    \n",
    "    # Combine candidates with their cross-encoder scores\n",
    "    scored_candidates = []\n",
    "    for candidate, ce_score in zip(candidates, relevance_scores):\n",
    "        scored_candidate = candidate.copy()\n",
    "        scored_candidate['cross_encoder_score'] = float(ce_score)\n",
    "        scored_candidates.append(scored_candidate)\n",
    "    \n",
    "    # Sort by cross-encoder score (highest first)\n",
    "    reranked = sorted(scored_candidates, \n",
    "                     key=lambda x: x['cross_encoder_score'], \n",
    "                     reverse=True)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Cross-encoder re-ranking completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return reranked[:top_k]\n",
    "\n",
    "# Test cross-encoder re-ranking on RRF results\n",
    "test_query = \"black hole formation from stellar collapse\"\n",
    "print(f\"\\nüîç Testing cross-encoder re-ranking for: '{test_query}'\")\n",
    "\n",
    "# Get RRF results as candidates for re-ranking\n",
    "hybrid_df = hybrid_retrieve(test_query, top_k_lex=15, top_k_sem=15)\n",
    "rrf_candidates = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "\n",
    "# Take top 20 RRF candidates for re-ranking (manageable size for cross-encoder)\n",
    "candidates_for_reranking = rrf_candidates[:20]\n",
    "\n",
    "print(f\"\\nüìä Re-ranking top {len(candidates_for_reranking)} RRF candidates\")\n",
    "\n",
    "# Apply cross-encoder re-ranking\n",
    "reranked_results = rerank_with_cross_encoder(\n",
    "    test_query, \n",
    "    candidates_for_reranking, \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# Display comparison: RRF order vs Cross-encoder order\n",
    "print(f\"\\nüìà Comparison: RRF vs Cross-Encoder ranking\")\n",
    "print(\"\\nRRF Ranking (before re-ranking):\")\n",
    "print(\"Rank\\tDoc ID\\tTitle\\t\\t\\t\\tRRF Score\")\n",
    "print(\"-\" * 70)\n",
    "for i, candidate in enumerate(candidates_for_reranking[:5], 1):\n",
    "    title_short = candidate['title'][:35] + '...' if len(candidate['title']) > 35 else candidate['title']\n",
    "    print(f\"{i}\\t{candidate['doc_id']}\\t{title_short:<38}\\t{candidate['rrf_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nCross-Encoder Ranking (after re-ranking):\")\n",
    "print(\"Rank\\tDoc ID\\tTitle\\t\\t\\t\\tCE Score\")\n",
    "print(\"-\" * 70)\n",
    "for i, result in enumerate(reranked_results[:5], 1):\n",
    "    title_short = result['title'][:35] + '...' if len(result['title']) > 35 else result['title']\n",
    "    print(f\"{i}\\t{result['doc_id']}\\t{title_short:<38}\\t{result['cross_encoder_score']:.4f}\")\n",
    "\n",
    "# Show detailed results with snippets\n",
    "print(f\"\\nüìã Top 5 results with snippets:\")\n",
    "for i, result in enumerate(reranked_results[:5], 1):\n",
    "    print(f\"\\n{i}. [{result['doc_id']}] {result['title']}\")\n",
    "    print(f\"   Cross-encoder score: {result['cross_encoder_score']:.4f}\")\n",
    "    print(f\"   Original RRF score: {result['rrf_score']:.4f}\")\n",
    "    \n",
    "    # Show first 200 characters as snippet\n",
    "    snippet = result['text'][:200] + '...' if len(result['text']) > 200 else result['text']\n",
    "    print(f\"   Snippet: {snippet}\")\n",
    "\n",
    "# Analyze ranking changes\n",
    "print(f\"\\nüîÑ Ranking changes analysis:\")\n",
    "rrf_order = {cand['doc_id']: i for i, cand in enumerate(candidates_for_reranking)}\n",
    "ce_order = {result['doc_id']: i for i, result in enumerate(reranked_results)}\n",
    "\n",
    "rank_changes = []\n",
    "for doc_id in ce_order.keys():\n",
    "    if doc_id in rrf_order:\n",
    "        change = rrf_order[doc_id] - ce_order[doc_id]  # Positive = moved up\n",
    "        rank_changes.append((doc_id, change))\n",
    "\n",
    "# Show biggest ranking changes\n",
    "rank_changes.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "print(\"Biggest ranking changes:\")\n",
    "for doc_id, change in rank_changes[:5]:\n",
    "    doc_title = next(r['title'] for r in reranked_results if r['doc_id'] == doc_id)\n",
    "    direction = \"‚Üë\" if change > 0 else \"‚Üì\" if change < 0 else \"=\"\n",
    "    print(f\"  [{doc_id}] {direction} {abs(change)} positions: {doc_title[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation: Bringing It All Together\n",
    "\n",
    "The **generation** step combines our retrieved and re-ranked documents with a language model to produce final answers. This is where RAG \"augments\" the generation with retrieved knowledge.\n",
    "\n",
    "### Key Components\n",
    "1. **Context Selection**: Choose top-k documents within token budget\n",
    "2. **Prompt Engineering**: Structure context and instructions clearly\n",
    "3. **Citation**: Enable traceability back to source documents\n",
    "4. **Grounding**: Instruct the model to use only provided context\n",
    "\n",
    "### Prompt Structure\n",
    "A well-structured RAG prompt includes:\n",
    "- **System message**: Instructions for behavior and citation\n",
    "- **Context section**: Retrieved documents with clear formatting\n",
    "- **Query**: User's original question\n",
    "- **Instructions**: Explicit grounding requirements\n",
    "\n",
    "### Token Budget Management\n",
    "Language models have context limits. We must:\n",
    "- Prioritize highest-ranked documents\n",
    "- Truncate or summarize if needed\n",
    "- Leave space for the generated response\n",
    "\n",
    "### Citation Strategy\n",
    "Include document IDs in the context so the model can reference specific sources. This enables fact-checking and builds user trust.\n",
    "\n",
    "**Environment handling**: Read API keys from environment variables and provide graceful fallbacks for missing credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end RAG pipeline with OpenAI generation\n",
    "# This combines all our retrieval components with final answer generation\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Rough estimation of token count (approximately 4 characters per token).\n",
    "    This is a simple heuristic; actual tokenization may differ.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        int: Estimated token count\n",
    "    \"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def select_context_chunks(ranked_results, max_tokens=2000):\n",
    "    \"\"\"\n",
    "    Select top-ranked documents that fit within token budget.\n",
    "    \n",
    "    Args:\n",
    "        ranked_results (list): Ranked documents from retrieval pipeline\n",
    "        max_tokens (int): Maximum tokens to use for context\n",
    "    \n",
    "    Returns:\n",
    "        list: Selected documents within token budget\n",
    "    \"\"\"\n",
    "    selected_chunks = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for result in ranked_results:\n",
    "        # Estimate tokens for this document (title + text + formatting)\n",
    "        doc_text = f\"[{result['doc_id']}] {result['title']}\\n{result['text']}\"\n",
    "        doc_tokens = estimate_tokens(doc_text)\n",
    "        \n",
    "        # Check if adding this document would exceed budget\n",
    "        if total_tokens + doc_tokens <= max_tokens:\n",
    "            selected_chunks.append(result)\n",
    "            total_tokens += doc_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return selected_chunks, total_tokens\n",
    "\n",
    "def create_rag_prompt(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Create a structured prompt for RAG generation.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question\n",
    "        context_chunks (list): Selected context documents\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (system_message, user_message)\n",
    "    \"\"\"\n",
    "    # System message with clear instructions\n",
    "    system_message = \"\"\"You are a helpful assistant that answers questions based on provided context. \n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the user's question using ONLY the information provided in the context below\n",
    "2. If you cite information, include the document ID in brackets [doc_id]\n",
    "3. If the context doesn't contain enough information to answer the question, say so clearly\n",
    "4. Be accurate and specific - don't make assumptions beyond what's stated in the context\n",
    "5. Provide a clear, well-structured answer\n",
    "\"\"\"\n",
    "    \n",
    "    # Format context documents clearly\n",
    "    context_text = \"\\n\\nCONTEXT DOCUMENTS:\\n\\n\"\n",
    "    for i, chunk in enumerate(context_chunks, 1):\n",
    "        context_text += f\"Document {i}: [{chunk['doc_id']}]\\n\"\n",
    "        context_text += f\"Title: {chunk['title']}\\n\"\n",
    "        context_text += f\"Content: {chunk['text']}\\n\\n\"\n",
    "    \n",
    "    # User message with query and context\n",
    "    user_message = f\"{context_text}\\nQUESTION: {query}\\n\\nPlease provide a comprehensive answer based on the context above.\"\n",
    "    \n",
    "    return system_message, user_message\n",
    "\n",
    "def answer_query(query_text, max_context_tokens=2000):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve, re-rank, and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): User's question\n",
    "        max_context_tokens (int): Maximum tokens for context\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete RAG results including retrieval steps and final answer\n",
    "    \"\"\"\n",
    "    print(f\"üîç Starting RAG pipeline for: '{query_text}'\")\n",
    "    \n",
    "    # Step 1: Hybrid retrieval (BM25 + Semantic)\n",
    "    print(\"üìä Step 1: Hybrid retrieval...\")\n",
    "    hybrid_results = hybrid_retrieve(query_text, top_k_lex=15, top_k_sem=15)\n",
    "    \n",
    "    # Step 2: Rank fusion with RRF\n",
    "    print(\"üîÑ Step 2: Rank fusion (RRF)...\")\n",
    "    rrf_results = apply_rrf_to_hybrid(hybrid_results, k=60)\n",
    "    \n",
    "    # Step 3: Cross-encoder re-ranking\n",
    "    print(\"üéØ Step 3: Cross-encoder re-ranking...\")\n",
    "    top_candidates = rrf_results[:20]  # Re-rank top 20 candidates\n",
    "    reranked_results = rerank_with_cross_encoder(query_text, top_candidates, top_k=15)\n",
    "    \n",
    "    # Step 4: Context selection within token budget\n",
    "    print(\"üìù Step 4: Context selection...\")\n",
    "    selected_context, context_tokens = select_context_chunks(reranked_results, max_context_tokens)\n",
    "    print(f\"   Selected {len(selected_context)} documents using ~{context_tokens} tokens\")\n",
    "    \n",
    "    # Step 5: Generate answer with OpenAI\n",
    "    print(\"ü§ñ Step 5: Generating answer...\")\n",
    "    \n",
    "    # Check for OpenAI API key\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        print(\"‚ö†Ô∏è  OpenAI API key not found in environment variables\")\n",
    "        print(\"   Set OPENAI_API_KEY environment variable to enable generation\")\n",
    "        return {\n",
    "            'query': query_text,\n",
    "            'retrieval_results': len(hybrid_results),\n",
    "            'rrf_results': len(rrf_results),\n",
    "            'reranked_results': len(reranked_results),\n",
    "            'selected_context': selected_context,\n",
    "            'context_tokens': context_tokens,\n",
    "            'answer': \"[Generation skipped: OpenAI API key not available]\",\n",
    "            'citations': [chunk['doc_id'] for chunk in selected_context]\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Initialize OpenAI client\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        \n",
    "        # Create RAG prompt\n",
    "        system_message, user_message = create_rag_prompt(query_text, selected_context)\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Fast, cost-effective model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            max_tokens=500,  # Limit response length\n",
    "            temperature=0.1  # Low temperature for factual responses\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during generation: {str(e)}\")\n",
    "        answer = f\"[Generation failed: {str(e)}]\"\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        'query': query_text,\n",
    "        'retrieval_results': len(hybrid_results),\n",
    "        'rrf_results': len(rrf_results),\n",
    "        'reranked_results': len(reranked_results),\n",
    "        'selected_context': selected_context,\n",
    "        'context_tokens': context_tokens,\n",
    "        'answer': answer,\n",
    "        'citations': [chunk['doc_id'] for chunk in selected_context]\n",
    "    }\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "test_queries = [\n",
    "    \"How do black holes form and what happens at the event horizon?\",\n",
    "    \"What are Python decorators and how do they modify function behavior?\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Testing complete RAG pipeline:\\n\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"TEST {i}: {query}\")\n",
    "    print(f\"=\" * 80)\n",
    "    \n",
    "    # Run complete RAG pipeline\n",
    "    rag_result = answer_query(query)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Pipeline Summary:\")\n",
    "    print(f\"   Hybrid retrieval: {rag_result['retrieval_results']} candidates\")\n",
    "    print(f\"   RRF fusion: {rag_result['rrf_results']} documents\")\n",
    "    print(f\"   Re-ranked: {rag_result['reranked_results']} documents\")\n",
    "    print(f\"   Context used: {len(rag_result['selected_context'])} documents ({rag_result['context_tokens']} tokens)\")\n",
    "    \n",
    "    print(f\"\\nüìö Context Documents:\")\n",
    "    for j, doc in enumerate(rag_result['selected_context'], 1):\n",
    "        print(f\"   {j}. [{doc['doc_id']}] {doc['title']}\")\n",
    "    \n",
    "    print(f\"\\nüí¨ Generated Answer:\")\n",
    "    print(rag_result['answer'])\n",
    "    print(f\"\\nüîó Citations: {', '.join(rag_result['citations'])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Approach: Vector Store vs Prompt-Embedded vs Local Files\n",
    "\n",
    "The choice of knowledge storage and retrieval architecture depends on your specific requirements:\n",
    "\n",
    "### Vector Stores (e.g., Pinecone, Weaviate, Chroma)\n",
    "**Best for**: Medium to large corpora, frequent updates, production systems\n",
    "- **Pros**: Optimized ANN search, metadata filtering, horizontal scaling, real-time updates\n",
    "- **Cons**: Additional infrastructure, cost, complexity\n",
    "- **Use when**: >10,000 documents, multiple users, frequent content updates\n",
    "\n",
    "### Prompt-Embedded Dataset\n",
    "**Best for**: Very small, static knowledge bases\n",
    "- **Pros**: Simplest implementation, no retrieval needed, perfect recall\n",
    "- **Cons**: Limited by context window, expensive tokens, no semantic search\n",
    "- **Use when**: <10 short documents, completely static content, maximum simplicity\n",
    "\n",
    "### Local File Embeddings (Our Approach)\n",
    "**Best for**: Small to medium corpora, single-node applications, development\n",
    "- **Pros**: No external dependencies, fast development, full control, offline capability\n",
    "- **Cons**: No horizontal scaling, manual index updates, limited concurrent access\n",
    "- **Use when**: <100,000 documents, single-node deployment, development/prototyping\n",
    "\n",
    "### Migration Path\n",
    "Start with local files for development, then migrate to a vector store when you need:\n",
    "- More than ~50,000 documents\n",
    "- Real-time updates\n",
    "- Multiple concurrent users\n",
    "- Advanced filtering capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of RAG Terms\n",
    "\n",
    "- **Document**: A single piece of content in your knowledge base (article, page, etc.)\n",
    "- **Chunk**: A segment of a document, typically 100-500 tokens for better embedding quality\n",
    "- **Corpus**: The complete collection of documents available for retrieval\n",
    "- **Index**: Data structure enabling fast search (TF-IDF matrix, embedding vectors, etc.)\n",
    "- **TF-IDF**: Term Frequency-Inverse Document Frequency; scores terms by frequency vs rarity\n",
    "- **BM25**: Best Matching 25; probabilistic ranking function improving on TF-IDF\n",
    "- **Embedding**: Dense vector representation capturing semantic meaning of text\n",
    "- **Vector Store**: Database optimized for storing and searching high-dimensional vectors\n",
    "- **ANN**: Approximate Nearest Neighbors; fast similarity search with slight accuracy trade-off\n",
    "- **FAISS**: Facebook AI Similarity Search; library for efficient similarity search\n",
    "- **Hybrid Retrieval**: Combining multiple retrieval methods (lexical + semantic)\n",
    "- **RRF**: Reciprocal Rank Fusion; method for combining rankings from multiple systems\n",
    "- **Cross-encoder**: Transformer model scoring (query, passage) pairs for re-ranking\n",
    "- **Top-k**: Retrieving the k highest-scoring results\n",
    "- **Recall**: Fraction of relevant documents successfully retrieved\n",
    "- **Precision**: Fraction of retrieved documents that are actually relevant\n",
    "- **Context Window**: Maximum input length a language model can process\n",
    "- **Hallucination**: When language models generate factually incorrect information\n",
    "- **Prompt Template**: Structured format for providing context and instructions to LLMs\n",
    "- **Grounding**: Ensuring model responses are based on provided evidence rather than training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running retrieval method evaluation...\n",
      "\n",
      "üìä Evaluating TF-IDF...\n",
      "üìä Evaluating BM25...\n",
      "üìä Evaluating Semantic...\n",
      "üìä Evaluating RRF...\n",
      "üìä Evaluating Cross-encoder...\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.06 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 19 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.10 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.10 seconds\n",
      "\n",
      "üìà Retrieval Method Comparison (Hit@K scores):\n",
      "Method          Hit@5    Hit@10  \n",
      "-----------------------------------\n",
      "TF-IDF          1.00     1.00    \n",
      "BM25            1.00     1.00    \n",
      "Semantic        1.00     1.00    \n",
      "RRF             1.00     1.00    \n",
      "Cross-encoder   1.00     1.00    \n",
      "\n",
      "üìä Analysis:\n",
      "üèÜ Best Hit@5: TF-IDF (1.00)\n",
      "üèÜ Best Hit@10: TF-IDF (1.00)\n",
      "\n",
      "üìà Improvements over TF-IDF baseline:\n",
      "   BM25: +0.00 Hit@5\n",
      "   Semantic: +0.00 Hit@5\n",
      "   RRF: +0.00 Hit@5\n",
      "   Cross-encoder: +0.00 Hit@5\n",
      "\n",
      "‚úÖ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Simple evaluation harness to compare retrieval methods\n",
    "# This provides quantitative comparison of different approaches\n",
    "\n",
    "# Define evaluation queries with expected relevant document IDs\n",
    "# These are hand-crafted based on our synthetic corpus\n",
    "evaluation_queries_simple = [\n",
    "    {\n",
    "        'query': 'black hole formation stellar collapse',\n",
    "        'relevant_docs': ['ast_001', 'ast_005']  # Black hole docs\n",
    "    },\n",
    "    {\n",
    "        'query': 'python decorators function modification',\n",
    "        'relevant_docs': ['py_002']  # Decorators doc\n",
    "    },\n",
    "    {\n",
    "        'query': 'exercise cardiovascular heart health',\n",
    "        'relevant_docs': ['health_002']  # Exercise and cardiovascular health\n",
    "    },\n",
    "    {\n",
    "        'query': 'cooking browning maillard reaction',\n",
    "        'relevant_docs': ['cook_001', 'cook_005']  # Maillard and browning docs\n",
    "    },\n",
    "    {\n",
    "        'query': 'trade routes silk road medieval',\n",
    "        'relevant_docs': ['hist_003', 'hist_005']  # Trade route docs\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_queries_mixed = [\n",
    "    # --- EASY (BM25-friendly) ---\n",
    "    {\n",
    "        \"query\": \"black hole formation stellar collapse\",\n",
    "        \"relevant_docs\": [\"ast_001\", \"ast_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"python decorators function modification\",\n",
    "        \"relevant_docs\": [\"py_002\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"exercise cardiovascular heart health\",\n",
    "        \"relevant_docs\": [\"health_002\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"cooking browning maillard reaction\",\n",
    "        \"relevant_docs\": [\"cook_001\", \"cook_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"trade routes silk road medieval\",\n",
    "        \"relevant_docs\": [\"hist_003\", \"hist_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"python list comprehension squares even numbers\",\n",
    "        \"relevant_docs\": [\"py_001\", \"py_006\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"exception handling try except finally cleanup\",\n",
    "        \"relevant_docs\": [\"py_004\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"industrial revolution steam engines factories urbanization\",\n",
    "        \"relevant_docs\": [\"hist_004\"]\n",
    "    },\n",
    "\n",
    "    # --- HARD (BM25-tricky) ---\n",
    "    {\n",
    "        \"query\": \"point of no return around a collapsed star\",\n",
    "        \"relevant_docs\": [\"ast_001\", \"ast_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"galactic cores hosting monsters millions of suns\",\n",
    "        \"relevant_docs\": [\"ast_001\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"runaway expansion powered by the mysterious 68 percent\",\n",
    "        \"relevant_docs\": [\"ast_004\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"not caramelization‚Äîthe browning that needs amino acids\",\n",
    "        \"relevant_docs\": [\"cook_001\", \"cook_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"heat moves by touch swirl and radiation in the kitchen\",\n",
    "        \"relevant_docs\": [\"cook_003\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"curl your fingers and rock the blade for uniform dice\",\n",
    "        \"relevant_docs\": [\"cook_002\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"teach immunity with safe rehearsals of pathogens\",\n",
    "        \"relevant_docs\": [\"health_001\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"night work of the brain: memory glue and waste cleanup\",\n",
    "        \"relevant_docs\": [\"health_004\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"caravans carried silk and scriptures across Eurasia‚Äôs arteries\",\n",
    "        \"relevant_docs\": [\"hist_003\", \"hist_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"the @ syntax to wrap behavior without touching the function\",\n",
    "        \"relevant_docs\": [\"py_002\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"envrionments like venv and conda to isolate projects\",\n",
    "        \"relevant_docs\": [\"py_003\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"base miles plus intervals to raise threshold in endurance\",\n",
    "        \"relevant_docs\": [\"sport_005\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ice baths compression and sleep as recovery levers\",\n",
    "        \"relevant_docs\": [\"sport_003\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_queries_hard = [\n",
    "    {\n",
    "        \"query\": \"point of no return around a collapsed star\",\n",
    "        \"relevant_docs\": [\"ast_001\", \"ast_005\"]  # event horizon + formation\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"galactic nuclei monsters weighing millions of suns\",\n",
    "        \"relevant_docs\": [\"ast_001\"]  # supermassive black holes\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"runaway cosmic expansion driven by the 68 percent stuff\",\n",
    "        \"relevant_docs\": [\"ast_004\"]  # dark energy fraction + acceleration\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"the 27% invisible scaffolding vs the 5% ordinary atoms\",\n",
    "        \"relevant_docs\": [\"ast_004\"]  # dark matter vs baryonic matter\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"worlds found by watching stars blink when a planet passes\",\n",
    "        \"relevant_docs\": [\"ast_003\"]  # transit photometry + Kepler\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"stellar infancy: gas-dust nurseries before the long main act\",\n",
    "        \"relevant_docs\": [\"ast_002\"]  # protostar ‚Üí main sequence\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"massive suns die loudly; small ones cool quietly‚Äîexplain\",\n",
    "        \"relevant_docs\": [\"ast_002\", \"ast_005\"]  # supernovae vs white dwarfs; collapse\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"not caramelization‚Äîname the browning with amino acids\",\n",
    "        \"relevant_docs\": [\"cook_001\", \"cook_005\"]  # Maillard vs caramelization\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"golden crust engineering: heat too low vs too high outcomes\",\n",
    "        \"relevant_docs\": [\"cook_001\"]  # temp control for Maillard\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"heat moves by touching, swirling, and radiant glow in kitchens\",\n",
    "        \"relevant_docs\": [\"cook_003\"]  # conduction/convection/radiation\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"sour cabbage craft needs salt, temp, and pH discipline\",\n",
    "        \"relevant_docs\": [\"cook_004\"]  # lactic fermentation controls\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"keep your digits curled; rock the blade for uniform cubes\",\n",
    "        \"relevant_docs\": [\"cook_002\"]  # knife skills safety + consistency\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"the @ sign trick: wrap a function without editing its body\",\n",
    "        \"relevant_docs\": [\"py_002\"]  # decorators\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"separate Python worlds so dependencies don‚Äôt brawl\",\n",
    "        \"relevant_docs\": [\"py_003\"]  # virtual environments\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"catch problems gracefully; always clean up afterward\",\n",
    "        \"relevant_docs\": [\"py_004\"]  # try/except/finally\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"iterate without hoarding memory‚Äîyield your way through data\",\n",
    "        \"relevant_docs\": [\"py_005\"]  # generators\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"double each item, or map then filter for clarity\",\n",
    "        \"relevant_docs\": [\"py_006\", \"py_001\"]  # list processing + list comprehensions\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"AHA weekly target: minutes that lift HDL and tame BP\",\n",
    "        \"relevant_docs\": [\"health_002\"]  # 150 minutes + HDL/BP effects\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"teach the body‚Äôs defenders with safe rehearsals of pathogens\",\n",
    "        \"relevant_docs\": [\"health_001\"]  # vaccines train immune system\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"night shift for the brain: waste clearance and memory glue\",\n",
    "        \"relevant_docs\": [\"health_004\"]  # sleep functions\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"mind mechanics of competition: imagery, goals, staying present\",\n",
    "        \"relevant_docs\": [\"sport_002\"]  # sports psychology tools\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ice tubs, squeeze sleeves, and lights-out: the recovery toolbox\",\n",
    "        \"relevant_docs\": [\"sport_003\"]  # recovery modalities + sleep\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"add weight over time and favor big multi-joint moves\",\n",
    "        \"relevant_docs\": [\"sport_004\"]  # progressive overload + compounds\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"train the engine: base miles plus intervals to raise the threshold\",\n",
    "        \"relevant_docs\": [\"sport_005\"]  # endurance + lactate threshold\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"fifty-three days, giant bombards, ancient walls undone\",\n",
    "        \"relevant_docs\": [\"hist_001\"]  # Constantinople siege details\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"movable type: the idea copier that turbocharged reform and science\",\n",
    "        \"relevant_docs\": [\"hist_002\"]  # printing press impacts\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"caravans of silk and scripture crossed Eurasia‚Äôs arteries\",\n",
    "        \"relevant_docs\": [\"hist_003\", \"hist_005\"]  # Silk Road commerce+culture\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"smokestacks, steam, and cities swelling‚Äîname the shift\",\n",
    "        \"relevant_docs\": [\"hist_004\"]  # Industrial Revolution\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"dekorators that time calls‚Äîhigher-order wrappers with @\",\n",
    "        \"relevant_docs\": [\"py_002\"]  # misspelling + synonyms\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"envrionments: venv/conda/pipenv to sandbox projects\",\n",
    "        \"relevant_docs\": [\"py_003\"]  # misspelling + tool names\n",
    "    }\n",
    "]\n",
    "\n",
    "def calculate_hit_at_k(retrieved_doc_ids, relevant_doc_ids, k):\n",
    "    \"\"\"\n",
    "    Calculate Hit@K: whether at least one relevant document appears in top-k results.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids (list): List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids (list): List of relevant document IDs\n",
    "        k (int): Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        float: 1.0 if hit, 0.0 if miss\n",
    "    \"\"\"\n",
    "    top_k_retrieved = set(retrieved_doc_ids[:k])\n",
    "    relevant_set = set(relevant_doc_ids)\n",
    "    \n",
    "    # Hit if intersection is non-empty\n",
    "    return 1.0 if top_k_retrieved.intersection(relevant_set) else 0.0\n",
    "\n",
    "def evaluate_retrieval_method(method_name, retrieval_function, eval_queries, k_values=[5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate a retrieval method on multiple queries.\n",
    "    \n",
    "    Args:\n",
    "        method_name (str): Name of the retrieval method\n",
    "        retrieval_function (callable): Function that takes query and returns ranked results\n",
    "        eval_queries (list): List of evaluation query dictionaries\n",
    "        k_values (list): List of k values for Hit@K calculation\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    results = {f'hit_at_{k}': [] for k in k_values}\n",
    "    \n",
    "    for eval_item in eval_queries:\n",
    "        query = eval_item['query']\n",
    "        relevant_docs = eval_item['relevant_docs']\n",
    "        \n",
    "        # Get retrieval results\n",
    "        retrieved_results = retrieval_function(query)\n",
    "        \n",
    "        # Extract document IDs from results\n",
    "        if method_name == 'Semantic':\n",
    "            # For semantic search, extract doc_id from chunk info\n",
    "            retrieved_doc_ids = [result[2]['doc_id'] for result in retrieved_results]\n",
    "        else:\n",
    "            # For TF-IDF and BM25, extract id from doc info\n",
    "            retrieved_doc_ids = [result[2]['id'] for result in retrieved_results]\n",
    "        \n",
    "        # Calculate Hit@K for each k value\n",
    "        for k in k_values:\n",
    "            hit = calculate_hit_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            results[f'hit_at_{k}'].append(hit)\n",
    "    \n",
    "    # Calculate average Hit@K scores\n",
    "    avg_results = {}\n",
    "    for k in k_values:\n",
    "        avg_results[f'hit_at_{k}'] = np.mean(results[f'hit_at_{k}'])\n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "def evaluate_hybrid_method(method_name, eval_queries, k_values=[5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate hybrid methods (RRF, Cross-encoder) on multiple queries.\n",
    "    \"\"\"\n",
    "    results = {f'hit_at_{k}': [] for k in k_values}\n",
    "    \n",
    "    for eval_item in eval_queries:\n",
    "        query = eval_item['query']\n",
    "        relevant_docs = eval_item['relevant_docs']\n",
    "        \n",
    "        if method_name == 'RRF':\n",
    "            # Get RRF results\n",
    "            hybrid_df = hybrid_retrieve(query, top_k_lex=15, top_k_sem=15)\n",
    "            rrf_results = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "            retrieved_doc_ids = [result['doc_id'] for result in rrf_results]\n",
    "        \n",
    "        elif method_name == 'Cross-encoder':\n",
    "            # Get cross-encoder re-ranked results\n",
    "            hybrid_df = hybrid_retrieve(query, top_k_lex=15, top_k_sem=15)\n",
    "            rrf_results = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "            reranked_results = rerank_with_cross_encoder(query, rrf_results[:20], top_k=15)\n",
    "            retrieved_doc_ids = [result['doc_id'] for result in reranked_results]\n",
    "        \n",
    "        # Calculate Hit@K for each k value\n",
    "        for k in k_values:\n",
    "            hit = calculate_hit_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            results[f'hit_at_{k}'].append(hit)\n",
    "    \n",
    "    # Calculate average Hit@K scores\n",
    "    avg_results = {}\n",
    "    for k in k_values:\n",
    "        avg_results[f'hit_at_{k}'] = np.mean(results[f'hit_at_{k}'])\n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "# Run evaluation on all methods\n",
    "print(\"üß™ Running retrieval method evaluation...\\n\")\n",
    "\n",
    "# Prepare retrieval functions\n",
    "retrieval_methods = {\n",
    "    'TF-IDF': lambda q: query_tfidf(q, top_k=10),\n",
    "    'BM25': lambda q: query_bm25(q, top_k=10),\n",
    "    'Semantic': lambda q: semantic_search(q, top_k=10)\n",
    "}\n",
    "\n",
    "# Evaluate individual methods\n",
    "evaluation_results = {}\n",
    "evaluation_queries = evaluation_queries_mixed  # Use hard queries for robust evaluation\n",
    "for method_name, retrieval_func in retrieval_methods.items():\n",
    "    print(f\"üìä Evaluating {method_name}...\")\n",
    "    results = evaluate_retrieval_method(method_name, retrieval_func, evaluation_queries)\n",
    "    evaluation_results[method_name] = results\n",
    "\n",
    "# Evaluate hybrid methods (these require special handling)\n",
    "print(f\"üìä Evaluating RRF...\")\n",
    "evaluation_results['RRF'] = evaluate_hybrid_method('RRF', evaluation_queries)\n",
    "\n",
    "print(f\"üìä Evaluating Cross-encoder...\")\n",
    "evaluation_results['Cross-encoder'] = evaluate_hybrid_method('Cross-encoder', evaluation_queries)\n",
    "\n",
    "# Display results in a comparison table\n",
    "print(f\"\\nüìà Retrieval Method Comparison (Hit@K scores):\")\n",
    "print(f\"{'Method':<15} {'Hit@5':<8} {'Hit@10':<8}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for method_name, results in evaluation_results.items():\n",
    "    hit_at_5 = results['hit_at_5']\n",
    "    hit_at_10 = results['hit_at_10']\n",
    "    print(f\"{method_name:<15} {hit_at_5:<8.2f} {hit_at_10:<8.2f}\")\n",
    "\n",
    "# Analysis\n",
    "print(f\"\\nüìä Analysis:\")\n",
    "best_hit5 = max(evaluation_results.items(), key=lambda x: x[1]['hit_at_5'])\n",
    "best_hit10 = max(evaluation_results.items(), key=lambda x: x[1]['hit_at_10'])\n",
    "\n",
    "print(f\"üèÜ Best Hit@5: {best_hit5[0]} ({best_hit5[1]['hit_at_5']:.2f})\")\n",
    "print(f\"üèÜ Best Hit@10: {best_hit10[0]} ({best_hit10[1]['hit_at_10']:.2f})\")\n",
    "\n",
    "# Show method improvements\n",
    "baseline_hit5 = evaluation_results['TF-IDF']['hit_at_5']\n",
    "print(f\"\\nüìà Improvements over TF-IDF baseline:\")\n",
    "for method_name, results in evaluation_results.items():\n",
    "    if method_name != 'TF-IDF':\n",
    "        improvement = results['hit_at_5'] - baseline_hit5\n",
    "        print(f\"   {method_name}: {improvement:+.2f} Hit@5\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation Framework with MRR and NDCG\n",
    "\n",
    "While Hit@K provides a basic measure of retrieval success, more sophisticated metrics offer deeper insights:\n",
    "\n",
    "### Mean Reciprocal Rank (MRR)\n",
    "MRR measures how high the first relevant document appears in the ranking. It gives more credit to systems that place relevant documents at the top.\n",
    "\n",
    "**Formula**: MRR = (1/|Q|) √ó Œ£(1/rank_i) where rank_i is the position of the first relevant document for query i\n",
    "\n",
    "### Normalized Discounted Cumulative Gain (NDCG)\n",
    "NDCG accounts for the position of all relevant documents and allows for graded relevance (not just binary relevant/irrelevant).\n",
    "\n",
    "**Benefits over Hit@K**:\n",
    "- More sensitive to ranking quality\n",
    "- Accounts for position of all relevant documents  \n",
    "- Provides finer-grained performance assessment\n",
    "- Better for comparing subtle differences between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing enhanced evaluation metrics with examples:\n",
      "\n",
      "üìä Example 1 - Perfect Ranking:\n",
      "   Retrieved: ['doc_a', 'doc_b', 'doc_c']...\n",
      "   Relevant: ['doc_a', 'doc_b']\n",
      "   MRR: 1.000\n",
      "   NDCG@5: 1.000\n",
      "   Precision@5: 0.400\n",
      "   Recall@5: 1.000\n",
      "   F1@5: 0.571\n",
      "\n",
      "üìä Example 2 - Poor Ranking (relevant docs at positions 4,5):\n",
      "   Retrieved: ['doc_x', 'doc_y', 'doc_z', 'doc_a', 'doc_b']\n",
      "   Relevant: ['doc_a', 'doc_b']\n",
      "   MRR: 0.250\n",
      "   NDCG@5: 0.501\n",
      "   Precision@5: 0.400\n",
      "   Recall@5: 1.000\n",
      "   F1@5: 0.571\n",
      "\n",
      "üí° Key Insights:\n",
      "   ‚Ä¢ MRR heavily penalizes when first relevant doc is ranked low\n",
      "   ‚Ä¢ NDCG accounts for position of ALL relevant documents\n",
      "   ‚Ä¢ Precision@K = relevant_retrieved / k_retrieved\n",
      "   ‚Ä¢ Recall@K = relevant_retrieved / total_relevant\n",
      "   ‚Ä¢ F1@K balances precision and recall\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "def calculate_mrr(retrieved_doc_ids: List[str], relevant_doc_ids: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank for a single query.\n",
    "    \n",
    "    MRR measures the quality of a ranking by looking at the position of the first relevant document.\n",
    "    Higher scores indicate that relevant documents appear earlier in the ranking.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids: List of known relevant document IDs\n",
    "    \n",
    "    Returns:\n",
    "        float: MRR score (1/rank of first relevant doc, or 0 if no relevant docs found)\n",
    "    \n",
    "    Example:\n",
    "        retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']\n",
    "        relevant = ['doc3', 'doc6']\n",
    "        MRR = 1/3 = 0.333 (first relevant doc 'doc3' is at position 3)\n",
    "    \"\"\"\n",
    "    relevant_set = set(relevant_doc_ids)\n",
    "    \n",
    "    for rank, doc_id in enumerate(retrieved_doc_ids, 1):\n",
    "        if doc_id in relevant_set:\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0  # No relevant documents found\n",
    "\n",
    "\n",
    "def calculate_dcg_at_k(retrieved_doc_ids: List[str], relevant_doc_ids: List[str], \n",
    "                       relevance_scores: Dict[str, float], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain at position k.\n",
    "    \n",
    "    DCG measures the usefulness of documents based on their position in the ranking,\n",
    "    with higher positions having exponentially more impact.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids: List of known relevant document IDs  \n",
    "        relevance_scores: Dict mapping doc_id to relevance score (0-3 scale typically)\n",
    "        k: Calculate DCG for top-k results\n",
    "    \n",
    "    Returns:\n",
    "        float: DCG@k score\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    \n",
    "    for i, doc_id in enumerate(retrieved_doc_ids[:k]):\n",
    "        if doc_id in relevance_scores:\n",
    "            relevance = relevance_scores[doc_id]\n",
    "            # DCG formula: rel_i / log2(i + 2) where i is 0-indexed position\n",
    "            dcg += relevance / math.log2(i + 2)\n",
    "    \n",
    "    return dcg\n",
    "\n",
    "\n",
    "def calculate_ndcg_at_k(retrieved_doc_ids: List[str], relevant_doc_ids: List[str],\n",
    "                        relevance_scores: Dict[str, float], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at position k.\n",
    "    \n",
    "    NDCG normalizes DCG by the ideal DCG (IDCG) to get a score between 0 and 1.\n",
    "    This allows fair comparison between queries with different numbers of relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids: List of known relevant document IDs\n",
    "        relevance_scores: Dict mapping doc_id to relevance score\n",
    "        k: Calculate NDCG for top-k results\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG@k score (0-1, where 1 is perfect ranking)\n",
    "    \"\"\"\n",
    "    # Calculate actual DCG\n",
    "    dcg = calculate_dcg_at_k(retrieved_doc_ids, relevant_doc_ids, relevance_scores, k)\n",
    "    \n",
    "    # Calculate Ideal DCG (IDCG) - what we'd get with perfect ranking\n",
    "    # Sort relevant docs by relevance score in descending order\n",
    "    ideal_ranking = sorted(relevance_scores.keys(), \n",
    "                          key=lambda x: relevance_scores[x], reverse=True)\n",
    "    idcg = calculate_dcg_at_k(ideal_ranking, relevant_doc_ids, relevance_scores, k)\n",
    "    \n",
    "    # NDCG = DCG / IDCG (avoid division by zero)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(retrieved_doc_ids: List[str], relevant_doc_ids: List[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K: fraction of retrieved documents that are relevant.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids: List of known relevant document IDs\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        float: Precision@k score (0-1)\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    top_k_retrieved = set(retrieved_doc_ids[:k])\n",
    "    relevant_set = set(relevant_doc_ids)\n",
    "    \n",
    "    relevant_retrieved = top_k_retrieved.intersection(relevant_set)\n",
    "    return len(relevant_retrieved) / k\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(retrieved_doc_ids: List[str], relevant_doc_ids: List[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@K: fraction of relevant documents that were retrieved.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order  \n",
    "        relevant_doc_ids: List of known relevant document IDs\n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        float: Recall@k score (0-1)\n",
    "    \"\"\"\n",
    "    if not relevant_doc_ids:\n",
    "        return 0.0\n",
    "        \n",
    "    top_k_retrieved = set(retrieved_doc_ids[:k])\n",
    "    relevant_set = set(relevant_doc_ids)\n",
    "    \n",
    "    relevant_retrieved = top_k_retrieved.intersection(relevant_set)\n",
    "    return len(relevant_retrieved) / len(relevant_set)\n",
    "\n",
    "\n",
    "def calculate_f1_at_k(retrieved_doc_ids: List[str], relevant_doc_ids: List[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate F1@K: harmonic mean of Precision@K and Recall@K.\n",
    "    \n",
    "    F1 provides a single score that balances precision and recall.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc_ids: List of retrieved document IDs in rank order\n",
    "        relevant_doc_ids: List of known relevant document IDs  \n",
    "        k: Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "        float: F1@k score (0-1)\n",
    "    \"\"\"\n",
    "    precision = calculate_precision_at_k(retrieved_doc_ids, relevant_doc_ids, k)\n",
    "    recall = calculate_recall_at_k(retrieved_doc_ids, relevant_doc_ids, k)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "# Create enhanced evaluation queries with graded relevance scores\n",
    "# For this demo, we'll use a simple binary relevance (relevant=1, not relevant=0)\n",
    "# In practice, you might have 3-point or 4-point relevance scales\n",
    "\n",
    "def create_relevance_scores(eval_queries: List[Dict]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Create relevance score mappings for evaluation queries.\n",
    "    \n",
    "    For simplicity, we use binary relevance: relevant docs get score 1.0, others get 0.0\n",
    "    In production, you might have multi-level relevance (0=irrelevant, 1=somewhat, 2=relevant, 3=highly relevant)\n",
    "    \"\"\"\n",
    "    relevance_mapping = {}\n",
    "    \n",
    "    for i, query_data in enumerate(eval_queries):\n",
    "        query_id = f\"query_{i}\"\n",
    "        relevance_scores = {}\n",
    "        \n",
    "        # All relevant docs get score 1.0, irrelevant docs get 0.0\n",
    "        for doc_id in query_data['relevant_docs']:\n",
    "            relevance_scores[doc_id] = 1.0\n",
    "            \n",
    "        relevance_mapping[query_id] = relevance_scores\n",
    "    \n",
    "    return relevance_mapping\n",
    "\n",
    "# Test the new metrics with a simple example\n",
    "print(\"üß™ Testing enhanced evaluation metrics with examples:\\n\")\n",
    "\n",
    "# Example 1: Perfect ranking\n",
    "retrieved_1 = ['doc_a', 'doc_b', 'doc_c', 'doc_d', 'doc_e']\n",
    "relevant_1 = ['doc_a', 'doc_b']\n",
    "relevance_1 = {'doc_a': 1.0, 'doc_b': 1.0}\n",
    "\n",
    "print(\"üìä Example 1 - Perfect Ranking:\")\n",
    "print(f\"   Retrieved: {retrieved_1[:3]}...\")\n",
    "print(f\"   Relevant: {relevant_1}\")\n",
    "print(f\"   MRR: {calculate_mrr(retrieved_1, relevant_1):.3f}\")\n",
    "print(f\"   NDCG@5: {calculate_ndcg_at_k(retrieved_1, relevant_1, relevance_1, 5):.3f}\")\n",
    "print(f\"   Precision@5: {calculate_precision_at_k(retrieved_1, relevant_1, 5):.3f}\")\n",
    "print(f\"   Recall@5: {calculate_recall_at_k(retrieved_1, relevant_1, 5):.3f}\")\n",
    "print(f\"   F1@5: {calculate_f1_at_k(retrieved_1, relevant_1, 5):.3f}\")\n",
    "\n",
    "# Example 2: Poor ranking  \n",
    "retrieved_2 = ['doc_x', 'doc_y', 'doc_z', 'doc_a', 'doc_b']\n",
    "relevant_2 = ['doc_a', 'doc_b']\n",
    "relevance_2 = {'doc_a': 1.0, 'doc_b': 1.0}\n",
    "\n",
    "print(f\"\\nüìä Example 2 - Poor Ranking (relevant docs at positions 4,5):\")\n",
    "print(f\"   Retrieved: {retrieved_2}\")\n",
    "print(f\"   Relevant: {relevant_2}\")\n",
    "print(f\"   MRR: {calculate_mrr(retrieved_2, relevant_2):.3f}\")\n",
    "print(f\"   NDCG@5: {calculate_ndcg_at_k(retrieved_2, relevant_2, relevance_2, 5):.3f}\")\n",
    "print(f\"   Precision@5: {calculate_precision_at_k(retrieved_2, relevant_2, 5):.3f}\")\n",
    "print(f\"   Recall@5: {calculate_recall_at_k(retrieved_2, relevant_2, 5):.3f}\")\n",
    "print(f\"   F1@5: {calculate_f1_at_k(retrieved_2, relevant_2, 5):.3f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ MRR heavily penalizes when first relevant doc is ranked low\")\n",
    "print(\"   ‚Ä¢ NDCG accounts for position of ALL relevant documents\")  \n",
    "print(\"   ‚Ä¢ Precision@K = relevant_retrieved / k_retrieved\")\n",
    "print(\"   ‚Ä¢ Recall@K = relevant_retrieved / total_relevant\")\n",
    "print(\"   ‚Ä¢ F1@K balances precision and recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running enhanced retrieval evaluation with multiple metrics...\n",
      "\n",
      "üìä Evaluating TF-IDF with enhanced metrics...\n",
      "üìä Evaluating BM25 with enhanced metrics...\n",
      "üìä Evaluating Semantic with enhanced metrics...\n",
      "üìä Evaluating RRF with enhanced metrics...\n",
      "üìä Evaluating Cross-encoder with enhanced metrics...\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 19 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.03 seconds\n",
      "üîÑ Computing cross-encoder scores for 20 candidates...\n",
      "‚è±Ô∏è  Cross-encoder re-ranking completed in 0.04 seconds\n",
      "\n",
      "üìà Comprehensive Retrieval Evaluation Results:\n",
      "Method          MRR    Hit@5  Hit@10  NDCG@5  NDCG@10  P@5    R@5    F1@5  \n",
      "--------------------------------------------------------------------------------\n",
      "TF-IDF          1.000  1.000  1.000   0.961   0.961    0.280  0.950  0.424 \n",
      "BM25            0.950  1.000  1.000   0.924   0.924    0.280  0.950  0.424 \n",
      "Semantic        0.925  1.000  1.000   0.935   0.935    0.300  1.000  0.452 \n",
      "RRF             0.950  1.000  1.000   0.924   0.946    0.280  0.950  0.424 \n",
      "Cross-encoder   1.000  1.000  1.000   1.000   1.000    0.300  1.000  0.452 \n",
      "\n",
      "üèÜ Best Performers by Metric:\n",
      "   Best MRR: TF-IDF (1.000)\n",
      "   Best NDCG@5: Cross-encoder (1.000)\n",
      "   Best F1@5: Semantic (0.452)\n",
      "\n",
      "üìä Metric Insights:\n",
      "   ‚Ä¢ MRR focuses on rank of first relevant document\n",
      "   ‚Ä¢ NDCG considers position of all relevant documents\n",
      "   ‚Ä¢ Hit@K is binary (found/not found)\n",
      "   ‚Ä¢ Precision@K shows relevance ratio in top-K\n",
      "   ‚Ä¢ Recall@K shows coverage of relevant documents\n",
      "   ‚Ä¢ F1@K balances precision and recall\n",
      "\n",
      "üìà Improvements over TF-IDF baseline:\n",
      "   BM25: MRR -0.050, NDCG@5 -0.037\n",
      "   Semantic: MRR -0.075, NDCG@5 -0.026\n",
      "   RRF: MRR -0.050, NDCG@5 -0.037\n",
      "   Cross-encoder: MRR +0.000, NDCG@5 +0.039\n",
      "\n",
      "‚úÖ Enhanced evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "def enhanced_evaluate_retrieval_method(method_name: str, retrieval_function, \n",
    "                                      eval_queries: List[Dict], k_values: List[int] = [5, 10]) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced evaluation using multiple metrics: Hit@K, MRR, NDCG, Precision, Recall, F1.\n",
    "    \n",
    "    Args:\n",
    "        method_name: Name of the retrieval method\n",
    "        retrieval_function: Function that takes query and returns ranked results\n",
    "        eval_queries: List of evaluation query dictionaries\n",
    "        k_values: List of k values for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing averaged scores for all metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'mrr': [],\n",
    "        **{f'hit_at_{k}': [] for k in k_values},\n",
    "        **{f'ndcg_at_{k}': [] for k in k_values},\n",
    "        **{f'precision_at_{k}': [] for k in k_values},\n",
    "        **{f'recall_at_{k}': [] for k in k_values},\n",
    "        **{f'f1_at_{k}': [] for k in k_values}\n",
    "    }\n",
    "    \n",
    "    # Create relevance scores for NDCG calculation\n",
    "    relevance_mapping = create_relevance_scores(eval_queries)\n",
    "    \n",
    "    for i, eval_item in enumerate(eval_queries):\n",
    "        query = eval_item['query']\n",
    "        relevant_docs = eval_item['relevant_docs']\n",
    "        query_id = f\"query_{i}\"\n",
    "        relevance_scores = relevance_mapping[query_id]\n",
    "        \n",
    "        # Get retrieval results\n",
    "        retrieved_results = retrieval_function(query)\n",
    "        \n",
    "        # Extract document IDs from results (handle different return formats)\n",
    "        if method_name == 'Semantic':\n",
    "            # For semantic search, extract doc_id from chunk info\n",
    "            retrieved_doc_ids = [result[2]['doc_id'] for result in retrieved_results]\n",
    "        else:\n",
    "            # For TF-IDF and BM25, extract id from doc info\n",
    "            retrieved_doc_ids = [result[2]['id'] for result in retrieved_results]\n",
    "        \n",
    "        # Calculate MRR (only needs to be calculated once per query)\n",
    "        mrr = calculate_mrr(retrieved_doc_ids, relevant_docs)\n",
    "        metrics['mrr'].append(mrr)\n",
    "        \n",
    "        # Calculate metrics for each k value\n",
    "        for k in k_values:\n",
    "            # Hit@K\n",
    "            hit = calculate_hit_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'hit_at_{k}'].append(hit)\n",
    "            \n",
    "            # NDCG@K\n",
    "            ndcg = calculate_ndcg_at_k(retrieved_doc_ids, relevant_docs, relevance_scores, k)\n",
    "            metrics[f'ndcg_at_{k}'].append(ndcg)\n",
    "            \n",
    "            # Precision@K\n",
    "            precision = calculate_precision_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'precision_at_{k}'].append(precision)\n",
    "            \n",
    "            # Recall@K\n",
    "            recall = calculate_recall_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'recall_at_{k}'].append(recall)\n",
    "            \n",
    "            # F1@K  \n",
    "            f1 = calculate_f1_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'f1_at_{k}'].append(f1)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_metrics = {}\n",
    "    for metric_name, values in metrics.items():\n",
    "        avg_metrics[metric_name] = np.mean(values)\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def enhanced_evaluate_hybrid_method(method_name: str, eval_queries: List[Dict], \n",
    "                                   k_values: List[int] = [5, 10]) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced evaluation for hybrid methods (RRF, Cross-encoder) using multiple metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'mrr': [],\n",
    "        **{f'hit_at_{k}': [] for k in k_values},\n",
    "        **{f'ndcg_at_{k}': [] for k in k_values}, \n",
    "        **{f'precision_at_{k}': [] for k in k_values},\n",
    "        **{f'recall_at_{k}': [] for k in k_values},\n",
    "        **{f'f1_at_{k}': [] for k in k_values}\n",
    "    }\n",
    "    \n",
    "    # Create relevance scores\n",
    "    relevance_mapping = create_relevance_scores(eval_queries)\n",
    "    \n",
    "    for i, eval_item in enumerate(eval_queries):\n",
    "        query = eval_item['query']\n",
    "        relevant_docs = eval_item['relevant_docs']\n",
    "        query_id = f\"query_{i}\"\n",
    "        relevance_scores = relevance_mapping[query_id]\n",
    "        \n",
    "        # Get method-specific results\n",
    "        if method_name == 'RRF':\n",
    "            hybrid_df = hybrid_retrieve(query, top_k_lex=15, top_k_sem=15)\n",
    "            rrf_results = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "            retrieved_doc_ids = [result['doc_id'] for result in rrf_results]\n",
    "        \n",
    "        elif method_name == 'Cross-encoder':\n",
    "            hybrid_df = hybrid_retrieve(query, top_k_lex=15, top_k_sem=15)\n",
    "            rrf_results = apply_rrf_to_hybrid(hybrid_df, k=60)\n",
    "            reranked_results = rerank_with_cross_encoder(query, rrf_results[:20], top_k=15)\n",
    "            retrieved_doc_ids = [result['doc_id'] for result in reranked_results]\n",
    "        \n",
    "        # Calculate MRR\n",
    "        mrr = calculate_mrr(retrieved_doc_ids, relevant_docs)\n",
    "        metrics['mrr'].append(mrr)\n",
    "        \n",
    "        # Calculate metrics for each k value\n",
    "        for k in k_values:\n",
    "            hit = calculate_hit_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'hit_at_{k}'].append(hit)\n",
    "            \n",
    "            ndcg = calculate_ndcg_at_k(retrieved_doc_ids, relevant_docs, relevance_scores, k)\n",
    "            metrics[f'ndcg_at_{k}'].append(ndcg)\n",
    "            \n",
    "            precision = calculate_precision_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'precision_at_{k}'].append(precision)\n",
    "            \n",
    "            recall = calculate_recall_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'recall_at_{k}'].append(recall)\n",
    "            \n",
    "            f1 = calculate_f1_at_k(retrieved_doc_ids, relevant_docs, k)\n",
    "            metrics[f'f1_at_{k}'].append(f1)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_metrics = {}\n",
    "    for metric_name, values in metrics.items():\n",
    "        avg_metrics[metric_name] = np.mean(values)\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# Run enhanced evaluation on all methods\n",
    "print(\"üöÄ Running enhanced retrieval evaluation with multiple metrics...\\n\")\n",
    "\n",
    "# Use mixed queries for comprehensive evaluation\n",
    "evaluation_queries = evaluation_queries_mixed[:10]  # Use subset for demo (faster execution)\n",
    "\n",
    "# Evaluate individual methods\n",
    "enhanced_results = {}\n",
    "retrieval_methods = {\n",
    "    'TF-IDF': lambda q: query_tfidf(q, top_k=10),\n",
    "    'BM25': lambda q: query_bm25(q, top_k=10),\n",
    "    'Semantic': lambda q: semantic_search(q, top_k=10)\n",
    "}\n",
    "\n",
    "for method_name, retrieval_func in retrieval_methods.items():\n",
    "    print(f\"üìä Evaluating {method_name} with enhanced metrics...\")\n",
    "    results = enhanced_evaluate_retrieval_method(method_name, retrieval_func, evaluation_queries)\n",
    "    enhanced_results[method_name] = results\n",
    "\n",
    "# Evaluate hybrid methods\n",
    "print(f\"üìä Evaluating RRF with enhanced metrics...\")\n",
    "enhanced_results['RRF'] = enhanced_evaluate_hybrid_method('RRF', evaluation_queries)\n",
    "\n",
    "print(f\"üìä Evaluating Cross-encoder with enhanced metrics...\")\n",
    "enhanced_results['Cross-encoder'] = enhanced_evaluate_hybrid_method('Cross-encoder', evaluation_queries)\n",
    "\n",
    "# Display comprehensive results table\n",
    "print(f\"\\nüìà Comprehensive Retrieval Evaluation Results:\")\n",
    "print(f\"{'Method':<15} {'MRR':<6} {'Hit@5':<6} {'Hit@10':<7} {'NDCG@5':<7} {'NDCG@10':<8} {'P@5':<6} {'R@5':<6} {'F1@5':<6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method_name, results in enhanced_results.items():\n",
    "    print(f\"{method_name:<15} \"\n",
    "          f\"{results['mrr']:<6.3f} \"\n",
    "          f\"{results['hit_at_5']:<6.3f} \"\n",
    "          f\"{results['hit_at_10']:<7.3f} \"\n",
    "          f\"{results['ndcg_at_5']:<7.3f} \"\n",
    "          f\"{results['ndcg_at_10']:<8.3f} \"\n",
    "          f\"{results['precision_at_5']:<6.3f} \"\n",
    "          f\"{results['recall_at_5']:<6.3f} \"\n",
    "          f\"{results['f1_at_5']:<6.3f}\")\n",
    "\n",
    "# Enhanced analysis\n",
    "print(f\"\\nüèÜ Best Performers by Metric:\")\n",
    "best_mrr = max(enhanced_results.items(), key=lambda x: x[1]['mrr'])\n",
    "best_ndcg5 = max(enhanced_results.items(), key=lambda x: x[1]['ndcg_at_5'])\n",
    "best_f1_5 = max(enhanced_results.items(), key=lambda x: x[1]['f1_at_5'])\n",
    "\n",
    "print(f\"   Best MRR: {best_mrr[0]} ({best_mrr[1]['mrr']:.3f})\")\n",
    "print(f\"   Best NDCG@5: {best_ndcg5[0]} ({best_ndcg5[1]['ndcg_at_5']:.3f})\")\n",
    "print(f\"   Best F1@5: {best_f1_5[0]} ({best_f1_5[1]['f1_at_5']:.3f})\")\n",
    "\n",
    "# Statistical significance insights\n",
    "print(f\"\\nüìä Metric Insights:\")\n",
    "print(f\"   ‚Ä¢ MRR focuses on rank of first relevant document\")\n",
    "print(f\"   ‚Ä¢ NDCG considers position of all relevant documents\")\n",
    "print(f\"   ‚Ä¢ Hit@K is binary (found/not found)\")\n",
    "print(f\"   ‚Ä¢ Precision@K shows relevance ratio in top-K\")\n",
    "print(f\"   ‚Ä¢ Recall@K shows coverage of relevant documents\")\n",
    "print(f\"   ‚Ä¢ F1@K balances precision and recall\")\n",
    "\n",
    "# Show improvement analysis\n",
    "baseline_method = 'TF-IDF'\n",
    "baseline_mrr = enhanced_results[baseline_method]['mrr']\n",
    "baseline_ndcg5 = enhanced_results[baseline_method]['ndcg_at_5']\n",
    "\n",
    "print(f\"\\nüìà Improvements over {baseline_method} baseline:\")\n",
    "for method_name, results in enhanced_results.items():\n",
    "    if method_name != baseline_method:\n",
    "        mrr_improvement = results['mrr'] - baseline_mrr\n",
    "        ndcg_improvement = results['ndcg_at_5'] - baseline_ndcg5\n",
    "        print(f\"   {method_name}: MRR {mrr_improvement:+.3f}, NDCG@5 {ndcg_improvement:+.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Enhanced Evaluation Metrics\n",
    "\n",
    "### When to Use Each Metric\n",
    "\n",
    "**Mean Reciprocal Rank (MRR)**\n",
    "- **Best for**: Systems where finding the first relevant document quickly is critical\n",
    "- **Example**: Question answering where users need one good answer\n",
    "- **Interpretation**: MRR=0.5 means on average, the first relevant document is at position 2\n",
    "\n",
    "**Normalized Discounted Cumulative Gain (NDCG)**  \n",
    "- **Best for**: Systems where ranking quality of all results matters\n",
    "- **Example**: Search engines where users browse multiple results\n",
    "- **Interpretation**: NDCG@5=0.8 means the ranking achieves 80% of the ideal score\n",
    "\n",
    "**Hit@K**\n",
    "- **Best for**: Simple binary assessment of retrieval success\n",
    "- **Example**: Basic \"did we find anything useful?\" evaluation\n",
    "- **Interpretation**: Hit@5=0.7 means 70% of queries had at least one relevant doc in top-5\n",
    "\n",
    "**Precision@K**\n",
    "- **Best for**: Systems where result quality (low false positives) is crucial\n",
    "- **Example**: Medical diagnosis support where wrong results are dangerous\n",
    "- **Interpretation**: P@5=0.6 means 60% of returned results are relevant\n",
    "\n",
    "**Recall@K**\n",
    "- **Best for**: Systems where completeness (low false negatives) is crucial  \n",
    "- **Example**: Legal discovery where missing documents has consequences\n",
    "- **Interpretation**: R@5=0.4 means we found 40% of all relevant documents\n",
    "\n",
    "**F1@K**\n",
    "- **Best for**: Balanced assessment of precision and recall\n",
    "- **Example**: General-purpose search systems\n",
    "- **Interpretation**: F1@5=0.5 balances finding relevant docs with avoiding irrelevant ones\n",
    "\n",
    "### Choosing the Right Metric for Your Use Case\n",
    "\n",
    "| Use Case | Primary Metric | Reasoning |\n",
    "|----------|---------------|-----------|\n",
    "| **QA Systems** | MRR | Users need one good answer fast |\n",
    "| **Research/Discovery** | NDCG@10 | Users explore multiple results |\n",
    "| **Fact Verification** | Precision@5 | Accuracy more important than completeness |\n",
    "| **Legal/Compliance** | Recall@10 | Can't afford to miss relevant documents |\n",
    "| **General Search** | F1@5 or NDCG@5 | Balance of multiple factors |\n",
    "\n",
    "### Statistical Significance Testing\n",
    "\n",
    "For production systems, always test statistical significance:\n",
    "- Use paired t-tests to compare methods\n",
    "- Require p < 0.05 for claiming improvements  \n",
    "- Test on diverse query sets (easy + hard queries)\n",
    "- Consider effect size, not just statistical significance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
